{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importin from Librarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U_tgbUuEyC85"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate , encoding , layer\n",
    "import math\n",
    "import pyspike as spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Neuron Model (PLIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GqocAxANsrvF"
   },
   "outputs": [],
   "source": [
    "class PLIFNode(neuron.BaseNode):\n",
    "    def __init__(self, init_tau=2.0, v_threshold=1.0, v_reset=0.0, surrogate_function=surrogate.ATan() ,  detach_reset = True ):\n",
    "        super().__init__(v_threshold, v_reset, surrogate_function)\n",
    "        init_w = - math.log(init_tau - 1)\n",
    "        self.w = nn.Parameter(torch.tensor(init_w, dtype=torch.float))\n",
    "        self.w.requires_grad = True\n",
    "        self.fire_mask = 0\n",
    "        self.spike = 0\n",
    "\n",
    "    def neuronal_charge(self, dv: torch.Tensor):\n",
    "        if self.v_reset is None:\n",
    "            # self.v += dv - self.v * self.w.sigmoid()\n",
    "            self.v += (dv - self.v) * self.w.sigmoid()\n",
    "        else:\n",
    "            # self.v += dv - (self.v - self.v_reset) * self.w.sigmoid()\n",
    "            self.v += (dv - (self.v - self.v_reset)) * self.w.sigmoid()\n",
    "\n",
    "    def neuronal_fire(self):\n",
    "        self.spike = self.surrogate_function(self.v - self.v_threshold)\n",
    "        return self.spike\n",
    "\n",
    "    def neuronal_reset(self , spike):\n",
    "        if self.v_reset is None:\n",
    "            self.v = self.v - spike * self.v_threshold\n",
    "        else:\n",
    "            self.v = (1. - spike) * self.v + spike * self.v_reset\n",
    "        \n",
    "\n",
    "    def tau(self):\n",
    "        return 1 / self.w.data.sigmoid().item()\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'v_threshold={self.v_threshold}, v_reset={self.v_reset}, tau={self.tau()}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating trainloader and testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PA3uGEDBg7g",
    "outputId": "6ddcafcd-da1d-4d5e-d010-d53f9eec2d39"
   },
   "outputs": [],
   "source": [
    "## Download datasets\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = \"'./data'\",\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "## DataLoaders\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_data_loader = data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creatin Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wvuM0dcoyG3j"
   },
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self , layer1_number = 50 , layer2_number = 10 , T = 10):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.count = 0\n",
    "        \n",
    "        self.layer1_number = layer1_number\n",
    "        self.layer2_number = layer2_number\n",
    "        self.T = T\n",
    "        \n",
    "        \n",
    "        self.s_layer1 = []\n",
    "        self.s_layer2 = []\n",
    "        self.s_layer3 = []\n",
    "        \n",
    "        self.flatten = layer.Flatten()\n",
    "        self.fc1 = layer.Linear(28*28, 50, bias=False) # Input\n",
    "        self.lif1 = LIFNode(surrogate_function=surrogate.ATan())\n",
    "        self.fc2 = layer.Linear(50, 10, bias=False) # Input\n",
    "        self.lif2 = LIFNode(surrogate_function=surrogate.ATan())\n",
    "        self.fc3 = layer.Linear(10, 10, bias=False) # Input\n",
    "        self.lif3 = LIFNode(surrogate_function=surrogate.ATan())\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.lif1(self.fc1(x))\n",
    "        self.s_layer1 = x\n",
    "        x = self.lif2(self.fc2(x))\n",
    "        self.s_layer2 = x \n",
    "        x = self.lif3(self.fc3(x))\n",
    "        self.s_layer3 = x\n",
    "\n",
    "        \n",
    "        return x , [self.s_layer1 , self.s_layer2 , self.s_layer3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike Train Recording Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bach_records_update(layer_activations, t , bach_records):\n",
    "    \n",
    "    for i , layer_activation in enumerate(layer_activations):\n",
    "\n",
    "        if t == 0 :\n",
    "    \n",
    "                layer_neurons_records = []\n",
    "                for _ in range(layer_activation.size(1)):\n",
    "                    layer_neurons_records.append(layer_activation[: , _].reshape(-1 , 1))\n",
    "    \n",
    "                bach_records.append(layer_neurons_records)\n",
    "    \n",
    "    \n",
    "        else :\n",
    "\n",
    "            for _ in range(layer_activation.size(1)):\n",
    "                bach_records[i][_] = torch.cat((bach_records[i][_] , layer_activation[: , _].reshape(-1 , 1)) ,  dim = 1)    \n",
    "    \n",
    "\n",
    "    return bach_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a3CFpzXYyAxE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2569166666666667\n",
      "0.4036\n",
      "0.49378333333333335\n",
      "0.5792666666666667\n",
      "0.5923166666666667\n",
      "0.59785\n"
     ]
    }
   ],
   "source": [
    "T= 10\n",
    "epochs = 6\n",
    "momentum = 0.9\n",
    "\n",
    "net = SNN()\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-3 )\n",
    "encoder = encoding.PoissonEncoder()\n",
    "\n",
    "all_records = []\n",
    "\n",
    "\n",
    "# Training\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    epoch_record = []\n",
    "\n",
    "    for img, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img\n",
    "        label = label\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "        \n",
    "        out_fr = 0\n",
    "\n",
    "        bach_records = [] # array of shape [*NumberOfLayers[(tensors , ... of shape (batchImageNumbers*Time) which represent every neuron in layer)]]\n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            encoded_img = encoder(img)\n",
    "            out_fr , layers_spikes = net(img)\n",
    "            bach_records = bach_records_update(layers_spikes , t , bach_records)\n",
    "\n",
    "        epoch_record.append(bach_records)\n",
    "            \n",
    "\n",
    "        \n",
    "        out_fr = out_fr/T\n",
    "        loss = F.mse_loss(out_fr, label_onehot) # + Synchrony\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel() # number of elemets\n",
    "        train_loss += loss.item()*label.numel()\n",
    "        # The correct rate is calculated as follows:\n",
    "        # The subscript i of the neuron with the highest firing rate in the output layer\n",
    "        # is considered as the result of classification.\n",
    "        train_acc += (out_fr.argmax(1)==label).float().sum().item()\n",
    "\n",
    "        # After optimizing the parameters, the state of the network should be reset\n",
    "        # because the neurons of the SNN have “memory”.\n",
    "        functional.reset_net(net)\n",
    "\n",
    "\n",
    "    all_records.append(epoch_record)\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples/(train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "    print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cython language_level=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HAVASET BE IN BASHE KE MOMKENE C EMPTY BRAGARDEH VAGHTI N1 SPIKY NADASHTE BASHE\n",
    "\n",
    "\n",
    "def Interpolate(a, b, t):\n",
    "\n",
    "    \n",
    "    if t < a and a < b:  return a\n",
    "    if t < b and b <= a: return b\n",
    "    if t > b:            return b    \n",
    "    return t               # interpolation\n",
    "\n",
    "\n",
    "def get_tau(spikes1, spikes2, i, j , max_tau , MRTS):\n",
    "\n",
    "\n",
    "    ## \"distances\" to neighbor: F/P=future/past, 1/2=N in spikesN.\n",
    "    mF1 = max_tau\n",
    "    mP1 = max_tau\n",
    "    mF2 = max_tau\n",
    "    mP2 = max_tau\n",
    "    \n",
    "    if i < len(spikes1)-1 and i > -1:\n",
    "        mF1 = (spikes1[i+1]-spikes1[i])\n",
    "    if j < len(spikes2)-1 and j > -1:\n",
    "        mF2 = (spikes2[j+1]-spikes2[j])\n",
    "    if i > 0:\n",
    "        mP1 = (spikes1[i]-spikes1[i-1])\n",
    "    if j > 0:\n",
    "        mP2 = (spikes2[j]-spikes2[j-1])\n",
    "\n",
    "    mF1, mF2, mP1, mP2 = mF1/2., mF2/2., mP1/2., mP2/2.\n",
    "    MRTS /= 4.\n",
    "\n",
    "    if i<0 or j<0 or spikes1[i] <= spikes2[j]:\n",
    "        s1F = Interpolate(mP1, mF1, MRTS)\n",
    "        s2P = Interpolate(mF2, mP2, MRTS)\n",
    "        return min(s1F, s2P)\n",
    "    else:\n",
    "        s1P = Interpolate(mF1, mP1, MRTS)\n",
    "        s2F = Interpolate(mP2, mF2, MRTS)\n",
    "        return min(s1P, s2F)\n",
    "\n",
    "\n",
    "def coincidence_single_profile_cython(spikestensor1, spikestensor2, t_start, t_end, max_tau, TotalRegularization , MRTS=0.):\n",
    "\n",
    "    spikes1 = ((torch.nonzero(spikestensor1)).view(1, -1)).numpy()[0]\n",
    "    spikes2 = ((torch.nonzero(spikestensor2)).view(1,-1)).numpy()[0]\n",
    "    \n",
    "\n",
    "    N1 = len(spikes1)\n",
    "    N2 = len(spikes2)\n",
    "\n",
    "    times = []\n",
    "    \n",
    "    if N2 == 0 :\n",
    "\n",
    "        spikes2 = np.append(spikes2 , np.array([100000]))\n",
    "        N2 = len(spikes2)\n",
    "    \n",
    "    j = -1\n",
    "    c = np.zeros(N1) \n",
    "    interval = t_end - t_start\n",
    "\n",
    "    true_max = t_end - t_start\n",
    "    if max_tau > 0:\n",
    "        true_max = min(true_max, 2*max_tau)\n",
    "\n",
    "    for i in range(N1):\n",
    "        \n",
    "        while j < N2-1 and spikes2[j+1] < spikes1[i]:\n",
    "            \n",
    "            # move forward until spikes2[j] is the last spike before spikes1[i]\n",
    "            # note that if spikes2[j] is after spikes1[i] we dont do anything\n",
    "            \n",
    "            j += 1\n",
    "\n",
    "        tau1 = get_tau(spikes1, spikes2, i, j, true_max, MRTS)\n",
    "\n",
    "        \n",
    "\n",
    "        if j > -1 and abs(spikes1[i]-spikes2[j]) < tau1: #حداقل یه اسپایک جی گیر اوردیم و به علاوه یک شده در واقع یه اسپایک به جز اسپایک اول قبل اسپایکمون بوده\n",
    "            c[i] = 1\n",
    "\n",
    "        if j < N2-1 and (j < 0 or spikes2[j] < spikes1[i]): \n",
    "            \n",
    "            # in case spikes2[j] is before spikes1[i] it has to be the one \n",
    "            # right before (see above), hence we move one forward and also \n",
    "            # check the next spike\n",
    "            \n",
    "            j += 1\n",
    "\n",
    "            tau2 = get_tau(spikes1, spikes2, i, j, true_max, MRTS)\n",
    "            \n",
    "            if abs(spikes2[j]-spikes1[i]) < tau2:\n",
    "                \n",
    "                # current spike in st1 is coincident\n",
    "                \n",
    "                c[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "        if c[i] == 0 :\n",
    "\n",
    "            part = spikestensor1[i]\n",
    "    \n",
    "            times.append([t for t in range(int(spikes1[i] - tau1 + 1) , int(spikes1[i] + tau2 + 1))])\n",
    "            for t in range(int(spikes1[i] - tau1 + 1) , int(spikes1[i] + tau2 + 1)) :\n",
    "\n",
    "\n",
    "                    part = part - (spikestensor2[t])\n",
    "            \n",
    "            \n",
    "            TotalRegularization +=  part**2\n",
    "            \n",
    "    return TotalRegularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 3, 1, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.poisson(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Spike Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonSpike:\n",
    "    def __init__(self, data, time=500, dt=0.1, max_freq=128, min_freq=0):\n",
    "        \"\"\"\n",
    "        Generate Poisson spike trains\n",
    "        :param data:\n",
    "        :param time:\n",
    "        :param dt:\n",
    "        :param max_freq:\n",
    "        :param min_freq\n",
    "        \"\"\"\n",
    "        data = np.array(data).reshape(-1)\n",
    "        self.data = (data - np.min(data.min())) / (np.max(data) - np.min(data)) if data.size != 1 else data\n",
    "        self.time = time\n",
    "        self.dt = dt\n",
    "\n",
    "        self.max_freq = max_freq\n",
    "        self.freq_data = self.data * (max_freq - min_freq) + min_freq\n",
    "        self.norm_data = 1000. / (self.freq_data + 1e-10)\n",
    "\n",
    "        fires = [\n",
    "            np.cumsum(np.random.poisson(cell / dt, (int(time / cell + 1)))) * dt\n",
    "            for cell in self.norm_data\n",
    "        ]\n",
    "        self.fires = np.array(fires, dtype=object)\n",
    "\n",
    "        self.spikes = np.zeros((data.shape[0], int(time/dt)))\n",
    "\n",
    "        for s, f in zip(self.spikes, self.fires):\n",
    "            f = f[f < time]  # round\n",
    "            s[np.array(f/dt, dtype=int)] = 1    # {0,1} spikes\n",
    "\n",
    "        self.monitor = {\n",
    "            's': self.spikes,\n",
    "            'f': self.fires,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regularization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNode(neuron.BaseNode):\n",
    "    def __init__(self, init_tau=2.0, v_threshold=1.0, v_reset=0.0, surrogate_function=surrogate.ATan() ,  detach_reset = True ):\n",
    "        super().__init__(v_threshold, v_reset, surrogate_function)\n",
    "        init_w = - math.log(init_tau - 1)\n",
    "        self.w = torch.tensor(init_w, dtype=torch.float)\n",
    "        self.fire_mask = 0\n",
    "        self.spike = 0\n",
    "\n",
    "    def neuronal_charge(self, dv: torch.Tensor):\n",
    "        if self.v_reset is None:\n",
    "            # self.v += dv - self.v * self.w.sigmoid()\n",
    "            self.v += (dv - self.v) * self.w.sigmoid()\n",
    "        else:\n",
    "            # self.v += dv - (self.v - self.v_reset) * self.w.sigmoid()\n",
    "            self.v += (dv - (self.v - self.v_reset)) * self.w.sigmoid()\n",
    "\n",
    "    def neuronal_fire(self):\n",
    "        self.spike = self.surrogate_function(self.v - self.v_threshold)\n",
    "        return self.spike\n",
    "\n",
    "    def neuronal_reset(self , spike):\n",
    "        if self.v_reset is None:\n",
    "            self.v = self.v - spike * self.v_threshold\n",
    "        else:\n",
    "            self.v = (1. - spike) * self.v + spike * self.v_reset\n",
    "        \n",
    "\n",
    "    def tau(self):\n",
    "        return 1 / self.w.data.sigmoid().item()\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'v_threshold={self.v_threshold}, v_reset={self.v_reset}, tau={self.tau()}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(nn.Module):\n",
    "    def __init__(self , layer1_number = 50  , T = 10):\n",
    "        super().__init__()        \n",
    "        \n",
    "        self.s_layer1 = []\n",
    "\n",
    "        self.fc1 = layer.Linear(layer1_number, 2, bias=False) # Input\n",
    "        self.weight = self.fc1.weight\n",
    "        self.lif1 = LIFNode(v_threshold = 0.2 , surrogate_function=surrogate.ATan())\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        x = self.lif1(self.fc1(x))\n",
    "        self.s_layer1 = x\n",
    "\n",
    "        \n",
    "        return x , [self.s_layer1] , self.weight\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cython language_level=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HAVASET BE IN BASHE KE MOMKENE C EMPTY BRAGARDEH VAGHTI N1 SPIKY NADASHTE BASHE\n",
    "\n",
    "\n",
    "def Interpolate(a, b, t):\n",
    "\n",
    "    \n",
    "    if t < a and a < b:  return a\n",
    "    if t < b and b <= a: return b\n",
    "    if t > b:            return b    \n",
    "    return t               # interpolation\n",
    "\n",
    "\n",
    "def get_tau(spikes1, spikes2, i, j , max_tau , MRTS):\n",
    "\n",
    "\n",
    "    ## \"distances\" to neighbor: F/P=future/past, 1/2=N in spikesN.\n",
    "    mF1 = max_tau\n",
    "    mP1 = max_tau\n",
    "    mF2 = max_tau\n",
    "    mP2 = max_tau\n",
    "    \n",
    "    if i < len(spikes1)-1 and i > -1:\n",
    "        mF1 = (spikes1[i+1]-spikes1[i])\n",
    "    if j < len(spikes2)-1 and j > -1:\n",
    "        mF2 = (spikes2[j+1]-spikes2[j])\n",
    "    if i > 0:\n",
    "        mP1 = (spikes1[i]-spikes1[i-1])\n",
    "    if j > 0:\n",
    "        mP2 = (spikes2[j]-spikes2[j-1])\n",
    "\n",
    "    mF1, mF2, mP1, mP2 = mF1/2., mF2/2., mP1/2., mP2/2.\n",
    "    MRTS /= 4.\n",
    "\n",
    "    if i<0 or j<0 or spikes1[i] <= spikes2[j]:\n",
    "        s1F = Interpolate(mP1, mF1, MRTS)\n",
    "        s2P = Interpolate(mF2, mP2, MRTS)\n",
    "        return min(s1F, s2P)\n",
    "    else:\n",
    "        s1P = Interpolate(mF1, mP1, MRTS)\n",
    "        s2F = Interpolate(mP2, mF2, MRTS)\n",
    "        return min(s1P, s2F)\n",
    "\n",
    "\n",
    "def coincidence_single_profile_cython(spikestensor1, spikestensor2, t_start, t_end, max_tau , MRTS=0.):\n",
    "\n",
    "\n",
    "    TotalRegularization = 0\n",
    "\n",
    "    spikes1 = ((torch.nonzero(spikestensor1)).view(1, -1)).numpy()[0]\n",
    "    spikes2 = ((torch.nonzero(spikestensor2)).view(1,-1)).numpy()[0]\n",
    "    \n",
    "\n",
    "    N1 = len(spikes1)\n",
    "    N2 = len(spikes2)\n",
    "\n",
    "    times = []\n",
    "    \n",
    "    if N2 == 0 :\n",
    "\n",
    "        spikes2 = np.append(spikes2 , np.array([10000000]))\n",
    "        N2 = len(spikes2)\n",
    "    \n",
    "    j = -1\n",
    "    c = np.zeros(N1) \n",
    "    interval = t_end - t_start\n",
    "\n",
    "    true_max = t_end - t_start\n",
    "    if max_tau > 0:\n",
    "        true_max = min(true_max, 2*max_tau)\n",
    "\n",
    "    for i in range(N1):\n",
    "\n",
    "        tau1 = 0\n",
    "        tau2 = 0\n",
    "        \n",
    "        while j < N2-1 and spikes2[j+1] < spikes1[i]:\n",
    "            \n",
    "            # move forward until spikes2[j] is the last spike before spikes1[i]\n",
    "            # note that if spikes2[j] is after spikes1[i] we dont do anything\n",
    "            \n",
    "            j += 1\n",
    "\n",
    "        tau1 = get_tau(spikes1, spikes2, i, j, true_max, MRTS)\n",
    "\n",
    "        \n",
    "\n",
    "        if j > -1 and abs(spikes1[i]-spikes2[j]) < tau1: #حداقل یه اسپایک جی گیر اوردیم و به علاوه یک شده در واقع یه اسپایک به جز اسپایک اول قبل اسپایکمون بوده\n",
    "            c[i] = 1\n",
    "\n",
    "        if j < N2-1 and (j < 0 or spikes2[j] < spikes1[i]): \n",
    "            \n",
    "            # in case spikes2[j] is before spikes1[i] it has to be the one \n",
    "            # right before (see above), hence we move one forward and also \n",
    "            # check the next spike\n",
    "            \n",
    "            j += 1\n",
    "\n",
    "            tau2 = get_tau(spikes1, spikes2, i, j, true_max, MRTS)\n",
    "            \n",
    "            if abs(spikes2[j]-spikes1[i]) < tau2:\n",
    "                \n",
    "                # current spike in st1 is coincident\n",
    "                \n",
    "                c[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "        if c[i] == 0 :\n",
    "\n",
    "            part = 1\n",
    "    \n",
    "            for t in range(int(spikes1[i] - tau1 + 1) , int(spikes1[i] + tau2 + 1)) :\n",
    "\n",
    "\n",
    "                if ( t_start <= t <= t_end ) :\n",
    "\n",
    "\n",
    "                    if spikes1[i] > t and abs(spikes1[i] - t) < tau1:\n",
    "\n",
    "                        part = part - (spikestensor2[t])\n",
    "                        times.append(t)\n",
    "\n",
    "\n",
    "                    elif spikes1[i] < t and abs(spikes1[i] - t) < tau2:\n",
    "\n",
    "                        \n",
    "                        part = part - (spikestensor2[t])\n",
    "                        times.append(t)\n",
    "\n",
    "                    \n",
    "                    elif spikes1[i] == t :\n",
    "\n",
    "                        part = part - (spikestensor2[t])\n",
    "                        times.append(t)\n",
    "\n",
    "\n",
    "\n",
    "            TotalRegularization +=  part**2\n",
    "\n",
    "\n",
    "        print(times)\n",
    "            \n",
    "    return TotalRegularization , N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(64., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(63., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(64., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(68., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(68., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "78\n",
      "total loss tensor(76., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(76., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "83\n",
      "total loss tensor(79., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(79., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "90\n",
      "total loss tensor(88., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(88., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "94\n",
      "total loss tensor(86., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(86., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "105\n",
      "total loss tensor(85., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(85., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "118\n",
      "total loss tensor(76., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(76., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "133\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(63., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "141\n",
      "total loss tensor(55., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(55., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "154\n",
      "total loss tensor(42., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "165\n",
      "total loss tensor(35., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "173\n",
      "total loss tensor(27., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(27., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "183\n",
      "total loss tensor(17., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(17., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "188\n",
      "total loss tensor(12., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(12., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "192\n",
      "total loss tensor(8., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(8., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "195\n",
      "total loss tensor(5., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(5., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "198\n",
      "total loss tensor(2., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(2., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "198\n",
      "total loss tensor(2., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(2., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "198\n",
      "total loss tensor(2., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(2., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "199\n",
      "total loss tensor(1., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(1., grad_fn=<AddBackward0>)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "spikes = torch.tensor(PoissonSpike(np.random.random(100),time=T,dt=1 , max_freq = 2500 , min_freq = 1800).spikes).float()\n",
    "network = test(layer1_number = 100)\n",
    "\n",
    "epoch_record = []\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 1e-3 )\n",
    "losses = []\n",
    "N = []\n",
    "\n",
    "\n",
    "for epoch in range(120):\n",
    "\n",
    "    network.train()\n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for t in range (T):\n",
    "\n",
    "        out_fr , layers_spikes , w = network(spikes[: , t])\n",
    "        \n",
    "        if t == 0 :\n",
    "\n",
    "            record = layers_spikes[0].reshape(-1 , 1)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            record = torch.cat((record , layers_spikes[0].reshape(-1 , 1)) , dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    loss1 , N1 = coincidence_single_profile_cython(record[0 , :], record[1 , :], 0 , 99 , max_tau = 2)\n",
    "    loss2 , N2 = coincidence_single_profile_cython(record[1 , :], record[0 , :], 0 , 99 , max_tau = 2)\n",
    "\n",
    "    loss = loss1 + loss2 \n",
    "\n",
    "\n",
    "    if loss == 0 :\n",
    "\n",
    "\n",
    "        losses.append(loss1+loss2)\n",
    "        N.append(N1+N2)\n",
    "\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "    \n",
    "\n",
    "    \n",
    "        print(N1+N2)\n",
    "        N.append(N1+N2)\n",
    "        losses.append(loss1+loss2)\n",
    "        loss.backward(retain_graph = True)\n",
    "        print(\"total loss\" , loss)\n",
    "        print(\"synchrony loss\" ,  loss1 + loss2)\n",
    "        print (record)\n",
    "        optimizer.step()\n",
    "        functional.reset_net(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spikingjelly import visualizing\n",
    "\n",
    "s_list = torch.cat((record[0 , :].reshape(-1,1) , record[1 , :].reshape(-1 , 1)) , dim = 1)\n",
    "s_list.shape\n",
    "\n",
    "figsize = (12, 8)\n",
    "dpi = 200\n",
    "\n",
    "visualizing.plot_1d_spikes(spikes=s_list.detach().numpy(), title='membrane sotentials', xlabel='simulating step',\n",
    "                        ylabel='neuron index', figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of unsynchroned neuron:  tensor(8.5000, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = []\n",
    "# Extract the values from the tensors\n",
    "for tensor in losses :\n",
    "\n",
    "    if tensor == 0 :\n",
    "\n",
    "        values.append(tensor)\n",
    "\n",
    "    else:\n",
    "\n",
    "        values.append(tensor.item())\n",
    "\n",
    "\n",
    "# Plot the values\n",
    "plt.plot(values, label='values')\n",
    "plt.plot(N, label='N1+N2')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Tensor Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"mean of unsynchroned neuron: \" , sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "total loss tensor(118., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(88., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "100\n",
      "total loss tensor(108., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(78., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "100\n",
      "total loss tensor(100., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(70., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "100\n",
      "total loss tensor(90., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(60., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "102\n",
      "total loss tensor(84.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(54., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "106\n",
      "total loss tensor(81.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(50., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "104\n",
      "total loss tensor(77.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(46., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "109\n",
      "total loss tensor(79.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(47., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "118\n",
      "total loss tensor(85.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(50., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "128\n",
      "total loss tensor(88.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(50., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "136\n",
      "total loss tensor(92.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(52., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "141\n",
      "total loss tensor(85.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "149\n",
      "total loss tensor(79.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "157\n",
      "total loss tensor(78.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "163\n",
      "total loss tensor(75.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(27., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "169\n",
      "total loss tensor(73.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(23., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "172\n",
      "total loss tensor(71.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(20., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "175\n",
      "total loss tensor(69.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(17., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "176\n",
      "total loss tensor(68.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(16., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "178\n",
      "total loss tensor(69.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(16., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "176\n",
      "total loss tensor(68.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(16., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "173\n",
      "total loss tensor(68.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(17., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "171\n",
      "total loss tensor(70.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(19., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "166\n",
      "total loss tensor(73.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(24., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "162\n",
      "total loss tensor(74.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(26., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "158\n",
      "total loss tensor(77.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "155\n",
      "total loss tensor(79.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "153\n",
      "total loss tensor(80.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "152\n",
      "total loss tensor(83.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "148\n",
      "total loss tensor(88.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(44., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "143\n",
      "total loss tensor(83.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "141\n",
      "total loss tensor(83.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "139\n",
      "total loss tensor(82.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(84.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(84.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "138\n",
      "total loss tensor(83.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(84.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(86.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(46., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(82.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(82.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(82.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(82.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(83.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(85.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(45., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "136\n",
      "total loss tensor(84.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(44., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "136\n",
      "total loss tensor(86.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(46., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "136\n",
      "total loss tensor(86.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(46., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "138\n",
      "total loss tensor(85.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(44., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "139\n",
      "total loss tensor(84.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "144\n",
      "total loss tensor(81.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(80.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(82.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(82.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(82.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(82.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "143\n",
      "total loss tensor(83.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "141\n",
      "total loss tensor(81.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "140\n",
      "total loss tensor(82., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "139\n",
      "total loss tensor(82.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "138\n",
      "total loss tensor(83.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(81.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(80.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(81., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(80.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(77.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(79., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "129\n",
      "total loss tensor(79.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(81., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(81., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(81.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "133\n",
      "total loss tensor(80.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(79.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(78.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(78.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(78.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(79.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "133\n",
      "total loss tensor(80.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(82.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(83., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(44., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(81.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(81.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(80.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "133\n",
      "total loss tensor(80.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "134\n",
      "total loss tensor(78.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "133\n",
      "total loss tensor(78.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(79.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(80.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "132\n",
      "total loss tensor(77.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(78.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "128\n",
      "total loss tensor(80.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "128\n",
      "total loss tensor(80.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "129\n",
      "total loss tensor(77.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(77., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "131\n",
      "total loss tensor(76.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "128\n",
      "total loss tensor(78.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "126\n",
      "total loss tensor(77.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "129\n",
      "total loss tensor(75.7000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "128\n",
      "total loss tensor(70.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "124\n",
      "total loss tensor(75.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "123\n",
      "total loss tensor(73.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "122\n",
      "total loss tensor(78.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "123\n",
      "total loss tensor(77.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "123\n",
      "total loss tensor(75.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "124\n",
      "total loss tensor(77.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "127\n",
      "total loss tensor(71.1000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "124\n",
      "total loss tensor(73.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "123\n",
      "total loss tensor(71.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "120\n",
      "total loss tensor(78., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "121\n",
      "total loss tensor(75.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "121\n",
      "total loss tensor(75.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "122\n",
      "total loss tensor(74.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "122\n",
      "total loss tensor(72.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "123\n",
      "total loss tensor(67.9000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "121\n",
      "total loss tensor(71.3000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "120\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "116\n",
      "total loss tensor(74.8000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "112\n",
      "total loss tensor(71.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "114\n",
      "total loss tensor(74.2000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "118\n",
      "total loss tensor(71.4000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 1., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "122\n",
      "total loss tensor(68.6000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 1., 1., 0.]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "spikes = torch.tensor(PoissonSpike(np.random.random(100),time=T,dt=1 , max_freq = 2500 , min_freq = 1800).spikes).float()\n",
    "network = test(layer1_number = 100)\n",
    "\n",
    "epoch_record = []\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 1e-3 )\n",
    "losses = []\n",
    "N = []\n",
    "\n",
    "\n",
    "for epoch in range(120):\n",
    "\n",
    "    network.train()\n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for t in range (T):\n",
    "\n",
    "        out_fr , layers_spikes , w = network(spikes[: , t])\n",
    "        \n",
    "        if t == 0 :\n",
    "\n",
    "            record = layers_spikes[0].reshape(-1 , 1)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            record = torch.cat((record , layers_spikes[0].reshape(-1 , 1)) , dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    loss1 , N1 = coincidence_single_profile_cython(record[0 , :], record[1 , :], 0 , 99 , max_tau = 2)\n",
    "    loss2 , N2 = coincidence_single_profile_cython(record[1 , :], record[0 , :], 0 , 99 , max_tau = 2)\n",
    "\n",
    "    loss = loss1 + loss2 + (0.3)*(torch.sum(torch.pow(record[0 , :], 2)) + torch.sum(torch.pow(record[1 , :], 2)))\n",
    "    \n",
    "\n",
    "    \n",
    "    print(N1+N2)\n",
    "    N.append(N1+N2)\n",
    "    losses.append(loss1+loss2)\n",
    "    loss.backward(retain_graph = True)\n",
    "    print(\"total loss\" , loss)\n",
    "    print(\"synchrony loss\" ,  loss1 + loss2)\n",
    "    print (record)\n",
    "    optimizer.step()\n",
    "    functional.reset_net(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.5000, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the values from the tensors\n",
    "values = [tensor.item() for tensor in losses]\n",
    "\n",
    "# Plot the values\n",
    "plt.plot(values, label='values')\n",
    "plt.plot(N, label='N1+N2')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Tensor Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print((sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spikingjelly import visualizing\n",
    "\n",
    "s_list = torch.cat((record[0 , :].reshape(-1,1) , record[1 , :].reshape(-1 , 1)) , dim = 1)\n",
    "s_list.shape\n",
    "\n",
    "figsize = (12, 8)\n",
    "dpi = 200\n",
    "\n",
    "visualizing.plot_1d_spikes(spikes=s_list.detach().numpy(), title='membrane sotentials', xlabel='simulating step',\n",
    "                        ylabel='neuron index', figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "total loss tensor(147., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(98., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "97\n",
      "total loss tensor(145.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(97., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "96\n",
      "total loss tensor(144., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(96., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1.]], grad_fn=<CatBackward0>)\n",
      "97\n",
      "total loss tensor(143.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(95., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1.]], grad_fn=<CatBackward0>)\n",
      "98\n",
      "total loss tensor(141., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(92., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1.]], grad_fn=<CatBackward0>)\n",
      "98\n",
      "total loss tensor(133., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(84., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "107\n",
      "total loss tensor(126.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(73., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "113\n",
      "total loss tensor(115.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(59., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "113\n",
      "total loss tensor(101.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(45., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "116\n",
      "total loss tensor(92., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "113\n",
      "total loss tensor(93.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "104\n",
      "total loss tensor(96., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(44., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "102\n",
      "total loss tensor(103., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(52., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "96\n",
      "total loss tensor(110., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(62., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "102\n",
      "total loss tensor(117., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(66., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "106\n",
      "total loss tensor(121., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(68., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "110\n",
      "total loss tensor(119., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(64., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "116\n",
      "total loss tensor(114., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(56., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "124\n",
      "total loss tensor(114., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(52., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "129\n",
      "total loss tensor(111.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(47., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "135\n",
      "total loss tensor(110.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "145\n",
      "total loss tensor(107.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "146\n",
      "total loss tensor(109., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "147\n",
      "total loss tensor(108.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "147\n",
      "total loss tensor(108.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "147\n",
      "total loss tensor(108.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "143\n",
      "total loss tensor(102.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "142\n",
      "total loss tensor(101., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "140\n",
      "total loss tensor(100., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "137\n",
      "total loss tensor(99.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "130\n",
      "total loss tensor(99., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "122\n",
      "total loss tensor(95., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "120\n",
      "total loss tensor(96., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "118\n",
      "total loss tensor(93., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "117\n",
      "total loss tensor(91.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "         0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "104\n",
      "total loss tensor(94., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "99\n",
      "total loss tensor(90.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "92\n",
      "total loss tensor(88., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "         1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "84\n",
      "total loss tensor(84., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "75\n",
      "total loss tensor(80.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(43., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(75., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(69., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "58\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(67.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "56\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(67.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(67.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "60\n",
      "total loss tensor(70., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(69.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(72.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(71.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(74.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(72., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(71., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "75\n",
      "total loss tensor(74.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "75\n",
      "total loss tensor(74.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "79\n",
      "total loss tensor(76.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "79\n",
      "total loss tensor(78.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "79\n",
      "total loss tensor(78.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(39., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "80\n",
      "total loss tensor(78., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "80\n",
      "total loss tensor(78., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "80\n",
      "total loss tensor(78., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(79., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(79., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(79., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "83\n",
      "total loss tensor(82.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(41., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(83., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(42., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(81., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "82\n",
      "total loss tensor(81., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(40., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "79\n",
      "total loss tensor(76.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "79\n",
      "total loss tensor(76.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "76\n",
      "total loss tensor(74., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "71\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(68., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "71\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "71\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(62., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(62., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(62.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(61., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(62.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(62.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(61., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(28., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(61., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(28., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(61.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(60., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(28., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(68.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(62.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(62., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(63., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(67., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(68., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(70.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(70.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(70.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(67.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(66., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(70., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(70., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(38., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "70\n",
      "total loss tensor(65., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(67., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(34., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(69.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(37., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(68., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(36., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(67.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(65.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(31., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "69\n",
      "total loss tensor(63.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "68\n",
      "total loss tensor(62., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(28., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(62., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(59.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(54., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(22., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(54.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(23., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(59.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "60\n",
      "total loss tensor(60., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(30., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(66.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(35., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(64.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(33., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(64., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(32., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "67\n",
      "total loss tensor(60.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(27., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "66\n",
      "total loss tensor(55., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(22., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(56.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(25., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "59\n",
      "total loss tensor(58.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(57.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "59\n",
      "total loss tensor(58.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(49., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(18., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(54., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(22., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "59\n",
      "total loss tensor(56.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(27., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(57.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "59\n",
      "total loss tensor(58.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(29., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "65\n",
      "total loss tensor(47.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(15., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(48., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(16., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "55\n",
      "total loss tensor(52.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(25., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "55\n",
      "total loss tensor(52.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(25., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "56\n",
      "total loss tensor(52., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(24., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "58\n",
      "total loss tensor(55., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(26., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "63\n",
      "total loss tensor(48.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(17., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(51.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(21., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "59\n",
      "total loss tensor(50.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(21., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(55.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(27., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "56\n",
      "total loss tensor(52., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(24., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(53.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(25., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(51.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(23., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "58\n",
      "total loss tensor(51., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(22., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "62\n",
      "total loss tensor(51., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(20., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "64\n",
      "total loss tensor(54., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(22., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "61\n",
      "total loss tensor(51.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(21., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "57\n",
      "total loss tensor(51.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(23., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "56\n",
      "total loss tensor(52., grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(24., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n",
      "53\n",
      "total loss tensor(47.5000, grad_fn=<AddBackward0>)\n",
      "synchrony loss tensor(21., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "         0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "spikes = torch.tensor(PoissonSpike(np.random.random(100),time=T,dt=1 , max_freq = 2500 , min_freq = 1800).spikes).float()\n",
    "network = test(layer1_number = 100)\n",
    "\n",
    "epoch_record = []\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 1e-3 )\n",
    "losses = []\n",
    "N = []\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    network.train()\n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for t in range (T):\n",
    "\n",
    "        out_fr , layers_spikes , w = network(spikes[: , t])\n",
    "        \n",
    "        if t == 0 :\n",
    "\n",
    "            record = layers_spikes[0].reshape(-1 , 1)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            record = torch.cat((record , layers_spikes[0].reshape(-1 , 1)) , dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    loss1 , N1 = coincidence_single_profile_cython(record[0 , :], record[1 , :], 0 , 99 , max_tau = 2)\n",
    "    loss2 , N2 = coincidence_single_profile_cython(record[1 , :], record[0 , :], 0 , 99 , max_tau = 2)\n",
    "\n",
    "    loss = loss1 + loss2 + (0.5)*(torch.sum(torch.pow(record[0 , :], 2)) + torch.sum(torch.pow(record[1 , :], 2)))\n",
    "    \n",
    "\n",
    "    \n",
    "    print(N1+N2)\n",
    "    N.append(N1+N2)\n",
    "    losses.append(loss1+loss2)\n",
    "    loss.backward(retain_graph = True)\n",
    "    print(\"total loss\" , loss)\n",
    "    print(\"synchrony loss\" ,  loss1 + loss2)\n",
    "    print (record)\n",
    "    optimizer.step()\n",
    "    functional.reset_net(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.4450, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the values from the tensors\n",
    "values = [tensor.item() for tensor in losses]\n",
    "\n",
    "# Plot the values\n",
    "plt.plot(values, label='values')\n",
    "plt.plot(N, label='N1+N2')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Tensor Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print((sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "from spikingjelly import visualizing\n",
    "\n",
    "s_list = torch.cat((record[0 , :].reshape(-1,1) , record[1 , :].reshape(-1 , 1)) , dim = 1)\n",
    "s_list.shape\n",
    "\n",
    "figsize = (12, 8)\n",
    "dpi = 200\n",
    "\n",
    "visualizing.plot_1d_spikes(spikes=s_list.detach().numpy(), title='membrane sotentials', xlabel='simulating step',\n",
    "                        ylabel='neuron index', figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.show()\n",
    "print(N[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of spikes 126\n",
      "synchrony loss tensor(68., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 120\n",
      "synchrony loss tensor(80., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 110\n",
      "synchrony loss tensor(90., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 105\n",
      "synchrony loss tensor(95., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n",
      "total number of spikes 100\n",
      "synchrony loss tensor(100., grad_fn=<AddBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "spikes = torch.tensor(PoissonSpike(np.random.random(100),time=T,dt=1 , max_freq = 2500 , min_freq = 1800).spikes).float()\n",
    "network = test(layer1_number = 100)\n",
    "\n",
    "epoch_record = []\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 1e-3 )\n",
    "losses = []\n",
    "N = []\n",
    "error = []\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    network.train()\n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for t in range (T):\n",
    "\n",
    "        out_fr , layers_spikes , w = network(spikes[: , t])\n",
    "        \n",
    "        if t == 0 :\n",
    "\n",
    "            record = layers_spikes[0].reshape(-1 , 1)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            record = torch.cat((record , layers_spikes[0].reshape(-1 , 1)) , dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    loss1 , N1 = coincidence_single_profile_cython(record[0 , :], record[1 , :], 0 , 99 , max_tau = 2)\n",
    "    loss2 , N2 = coincidence_single_profile_cython(record[1 , :], record[0 , :], 0 , 99 , max_tau = 2)\n",
    "    loss = loss1 + loss2\n",
    "\n",
    "\n",
    "    # Define the Gaussian kernel\n",
    "    kernel_size = 5\n",
    "    sigma = 1.0\n",
    "    kernel = torch.exp(-(torch.arange(kernel_size) - kernel_size // 2) ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    # Normalize the kernel\n",
    "    kernel = kernel / kernel.sum()\n",
    "    \n",
    "    \n",
    "    # Add a dimension to match the tensor shape\n",
    "    kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply the convolution\n",
    "    vector1 = (F.conv1d(record[0 , :].unsqueeze(0).unsqueeze(0), kernel)).squeeze()\n",
    "    vector2 = (F.conv1d(record[1 , :].unsqueeze(0).unsqueeze(0), kernel)).squeeze()\n",
    "    \n",
    "    #dot product of vector1 and vector2\n",
    "\n",
    "    dotproduct = torch.dot(vector1 , vector2)\n",
    "\n",
    "    loss = dotproduct - torch.norm(vector1)*torch.norm(vector2)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"total number of spikes\" , N1+N2)\n",
    "    N.append(N1+N2)\n",
    "    losses.append(loss)\n",
    "    loss.backward(retain_graph = True)\n",
    "    print(\"synchrony loss\" ,  loss1 + loss2)\n",
    "    print (record)\n",
    "    optimizer.step()\n",
    "    functional.reset_net(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values = []\n",
    "# Extract the values from the tensors\n",
    "for tensor in losses :\n",
    "\n",
    "    if tensor == 0 :\n",
    "\n",
    "        values.append(tensor.item())\n",
    "\n",
    "    else:\n",
    "\n",
    "        values.append(tensor.item())\n",
    "\n",
    "\n",
    "# Plot the values\n",
    "plt.plot(values, label='number of un-synchroned spikes')\n",
    "plt.plot(N, label='total number of spikes')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Tensor Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACUsAAAYsCAYAAAA1Ik63AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAB7CAAAewgFu0HU+AAC/LUlEQVR4nOzdd5hV1b3w8d/QB4aiiOUCFhBlFCkqzQKIPVETr1xiSVQ0FqLGqIk1MfH6RmMsKV6jYoNorr0lGl8TFVBQqoCiCIqgIKiAGDoOzH7/4OVkRqbCwCDr83kenmfPnHX2XmfPGZF9vmedvCzLsgAAAAAAAAAAANjG1antCQAAAAAAAAAAAGwJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAAAAAAAAACAJYikAAABKGTFiROTl5UVeXl7069evtqcD24T1v1N5eXlb7JhDhw7NHfPMM8/cYscFAACAiIhFixbFddddFz179oztttsu6tatm/t36tChQyMiYvbs2bnv7b777jV6/M25b+CbrV5tTwAAAAAAAAAA2HZ8+OGH0adPn/jkk09qeyoAG7CyFAAAAFDjzjzzzA3eLbqtqI1VogAAAOCb5LzzzsuFUvn5+XH88cfH+eefHxdccEFccMEFUVhYWMszZEvo169f7hrKiBEjans6kGNlKQAAAAAAAACgRsyfPz9eeumliIho2LBhTJkyJTp06FDLswL4N7EUAAAAwGaWZVltTwEAAAC2iEmTJuW2Dz300ApDqd13332z/Zt5c+4b+GbzMXwAAAAAAAAAQI1YvHhxbnuXXXapxZkAlE0sBQAAAAAAAADUiKKiotx2nTqSBGDr479MAADAN1peXl7uz3qTJ0+OwYMHx9577x0FBQVRUFAQPXv2jD/96U+xZs2aDfYxYcKEOPPMM6OwsDCaNGkSLVu2jMMOOyz+8pe/VHs+48ePj0suuSS6du0arVq1igYNGsTOO+8cffv2jZtuuqnUO+vKs/vuu+ce0+zZsyMi4oMPPoif/exn0alTp2jevHnk5+dHly5d4oYbbogVK1ZssI/p06fHhRdeGPvtt180a9YsWrRoEb169Yo77rgj1q5dW+3HlWVZPPXUU3HCCSfEbrvtFo0aNYqdd945jjrqqPjzn/8cxcXFle6jrJ/VlClT4uKLL45OnTrF9ttvH3l5efHd7353g/tOnDgxbrzxxjjuuOOiXbt2UVBQEA0aNIiddtopDjrooLjmmmvi448/rtJjKev8zp07N37xi19Ely5dokWLFtGkSZPo2LFjXHTRRfHRRx9Vab/rFRUVxYMPPhgDBw6Mdu3aRdOmTaNJkyaxxx57xCmnnBJPP/10jS8BX1RUFA899FD853/+Z+781KtXL5o2bRp77rlnHH300XHttdfGuHHjKt1XlmXx+OOPxymnnBLt27fP/Q61b98+Tj311HjiiScqnP/68zts2LDc9wYNGlTq57/+z69+9asKH9OmnscRI0bkjtWvX7/c91955ZU4+eSTo127dtGoUaNo2bJl9OnTJ/7nf/6n1AXd8vZVUlmPq+Rzq6xxFfn888/jgQceiDPOOCO6desW22+/fdSvXz9atGgRHTt2jEGDBsWLL75Y4T42xvjx4+PCCy+M/fffP7bbbruoV69e5Ofnxy677BK9evWKwYMHx2OPPRbLly+v8WMDAADwzVfy382DBg3KfX/YsGEb/Jv5zDPPzN0+e/bs3Pd33333cve/MdeVNmXf06dPj5/85CdRWFgYBQUF0axZs+jSpUtcddVVsXDhwmqdm8cffzyOP/74aN26dTRs2DDatGkTRx55ZAwbNix3rfLMM8/MzWHo0KHV2n9VH9OWvA63/tgjR47Mfe+www4r8xpKRY93+fLlceedd8bxxx8fu+22WzRu3DiaNm0aHTp0iLPOOiteeeWVap8biIiIDAAA4BssInJ/sizLbrrppqxu3bqlvl/yz9FHH52tWrUqy7IsW7NmTTZ48OByx0ZEdvLJJ2dr1qypdB5ffPFFdtJJJ1W4r4jIWrRokT3++OMV7mu33XbLjZ81a1b24IMPZo0bNy53n926dcu++OKL3P2vv/76rE6dOuWO79evX7Z8+fJyjz98+PDc2L59+2ZLlizJvvOd71T4uHr37p199tln1fpZ/fKXvyzzZ/Wd73yn1P26d+9e6XmNiKx+/frZTTfdVMlPasPz+/TTT2fNmzcvd7/5+fnZc889V+l+15+79u3bVzrXXr16ZXPnzq3SPiszffr0rLCwsErnKCKy999/v9x9zZgxI+vWrVul+zjggAOymTNnlrmPkue3sj+//OUvy9xHTZ3Hrz+XV69enZ1zzjkV7nP//ffPFixYUOG+qvJn1qxZpe7/9ed/Wf7whz9U+N+vkn/69++fLVy4sNx9ZVmWPfDAA7nxZ5xxRpljioqKsnPPPbfKj+uaa66p8JgAAACkqTr/bi75b9RZs2blvr/bbruVu/+Nua60sfu+8847s4YNG5Y7/5YtW2bjx4+v9Jx8+eWX2ZFHHlnhuTj44IOz+fPnZ2eccUbuew888ECl+67MxpyvLKu563DVuYZS3uN97LHHsp133rnS+x933HHZl19+ucnnjLTUCwAAgG3E3XffHVdccUVERHTu3Dm6du0adevWjbFjx8a7774bEREvvvhi/PjHP4677747fvSjH8WQIUOiTp060b179ygsLIzi4uJ47bXXYtasWRER8cgjj0SXLl3iyiuvLPe4n376afTv3z+mTZuW+96+++4bXbp0iYKCgvj888/jtddei0WLFsWXX34ZAwcOjAcffDBOO+20Sh/TCy+8EBdeeGEUFxdHhw4dokePHtGoUaN46623Yvz48RERMWnSpDj55JPjxRdfjBtvvDF+8Ytf5M5Bly5dol69ejFu3Lh45513ImLdO/0uvfTSuOuuu6p0Xs8888x49tlnIy8vL3r06BH77LNPrF69Ol5//fXc6jlvvPFGHH744TF69Oho1qxZpfu8+eab47rrrouIiPbt20ePHj2icePGMXv27Khfv36psevfqdawYcPYd999Y88994zmzZtHlmUxf/78GDt2bCxcuDCKiopyP//LL7+8So/tpZdeivPPPz/Wrl0bu+66a/Tu3TuaNWsWs2bNihEjRsSaNWti5cqVMXDgwJg6dWrsscce5e7r8ccfj9NOOy23MlF+fn706tUrdt9996hTp07MmDEj3njjjVizZk2MGTMmevfuHePHj4+ddtqpSnMty9KlS+OII46IOXPmRMS6pe27deuWe9fjihUr4pNPPokpU6ZU+q7HadOmRd++fWPBggW57+23337RtWvXyMvLi0mTJsXbb78dEeveYXjQQQfFq6++GnvttVep/ZxxxhmxaNGiePnll+O9996LiIjDDz88OnbsuMExe/ToscH3Nud5PPfcc2PYsGFRp06d6NmzZ3Ts2DGKi4tjzJgxMX369IiIePPNN+P000+Pv//976Xu27p167jgggsiIuKOO+7IfX/9976uKr8HXzdv3rzcym/t2rWLwsLCaNWqVTRq1Ci+/PLLePvtt3O/x6+88kocccQRMWbMmGjYsGG1j7Xez372sxgyZEju69atW0ePHj2iVatWUVxcHIsWLYp33303d34AAACgLCX/3fzee+/Fyy+/HBERHTt2jMMPP7zU2F69em3SsapzXam6hg4dGoMHD46IiL333jsOPPDAyM/Pj/feey9Gjx4dWZbFokWL4oQTTohp06ZF8+bNy9zP6tWr45hjjokxY8bkvvcf//Efceihh0ZBQUHMnDkzRo0aFaNHj86tFL651MZ1uPXPhaeffjrmzZsXERHf/e53o3Xr1huMLSws3OB7v/vd7+Kyyy7LrSrerFmz6N27d7Rp0ybWrl0b77zzTkyYMCGyLIvnnnsu+vXrF6NHj47GjRtv7GkiNbWaagEAAGyiKPEuooYNG2Y777xzNnz48A3G3XLLLblx9erVy2677bYsIrLCwsJs8uTJpcauWbMm+8lPfpIbX1BQkC1btqzM469duzY77LDDcmN79OiRvfnmmxuMW7lyZfarX/0qy8vLyyIia9KkSfbhhx+Wuc+SK/M0bNgwa9q0aZmrUT3yyCOl3hH2u9/9Lqtbt272H//xH9mIESM2GH/rrbfmxtapU2eDlW/WK/lOwAYNGmQRke2xxx5lvmPunnvuyerXr58bf+6555a5zywr/bOqV69e1rx58+zpp5/eYNz6lb/WGzx4cPb8889nK1asKHO/a9asyR544IGsSZMmuXe2lXdus2zD89ukSZPswQcfzIqLi0uNmzp1ata6devc2EGDBpW7z6lTp2b5+flZRGR5eXnZT3/602zx4sUbjJs5c2Z2yCGH5PZ57LHHlrvPqvj973+f29c+++yTvffee2WOKy4uzsaNG5cNHjw4+/jjjze4ffXq1VmXLl1y+9pxxx2zf/7znxuMe/HFF7MddtghN27//ffPvvrqqzKPuTHviKzp81jyubz+HaHdu3fPpk2bVmpccXFxqXMZEdnIkSPLnWfJcVVVlfvcd9992e23317hallTpkzJDjzwwNy+rr/++nLHVray1MKFC7N69eplEZHVrVs3Gzp06Aa/B+vNmzcv++Mf/5jde++95T9IAAAAyKq20vF6G7P6U1WvK23Mvhs2bJi1atUqe+GFFzYYN3LkyKxZs2a5sdddd125+/z5z39e6jrcLbfckq1du7bUmJkzZ2Y9evQodd2iOtdRKrK1XIfr27dvbh5lXbMty0svvZRbNb9BgwbZb37zmzJXyZ80aVK2zz775PY/ePDgKu0fsizLxFIAAMA3Wsl/+Ddq1CibOnVquWOPOOKIUuN33HHHcj86bs2aNdnee++dG/voo4+WOe7Pf/5zbkyvXr3KvZCw3i9/+cvc+PPPP7/MMSVjnry8vDKjlfV++MMflnpM+fn52bvvvlvu+JLnoLylsr++bHqTJk2yDz74oNx93nvvvaXmW97YkvusU6dOhTHKxnjkkUdy+7/88svLHff181vWxa/1nnvuuVLRXFFRUZnj+vfvnxt32223VTjPZcuWlbqQM2bMmKo9wDKU/OjHip4nlbn//vtz+6lfv36Zwd9648aNywU2EZENGzaszHEbE0vV9Hn8+nO5Q4cO2dKlS8vd54ABAyr9/cyyzRdLVdWXX36ZW4Z+l112KfejQiu7OP23v/0td/tpp522yfMCAACALNv8sVRVryttbCw1ZcqUcsf+z//8T25sx44dyxzzxRdfZI0aNcqNu/HGG8vd3+LFi0tdq9ocsVRtXoerbiy1du3arEOHDrn7PPXUUxWOnz9/frbTTjvlrmnNmTOnug+FRNUJAACAbcR5550X++67b7m3n3LKKaW+vvrqq2PHHXcsc2zdunVj4MCBua/HjRtX5rjbbrstt33XXXdFfn5+hXO88soro0WLFhER8fDDD0dxcXGF40844YQ44ogjyr3964/pvPPOK3Pp6rLGl/eYvu7SSy+N9u3bl3v72WefHQcccEBERGRZFvfee2+l+xwwYED06dOnSsevqgEDBkRBQUFErPt4vao47rjj4phjjin39m9961ux8847R0TEsmXLSn3U4npTpkyJV155JSIiunXrFj/5yU8qPGaTJk1yH5UYEfGXv/ylSnMty5IlS3LbrVq12uj93H333bntwYMHR7du3cod27179zjnnHNyX995550bfdyStsR5/M1vfpN7jpTlrLPOym1X9fejNjRv3jxOPPHEiIiYP39+7mNGq6umnj8AAACwJW2O60rrnXvuudG5c+dybz/99NOjXr16ERExffr0Uv+2Xu9///d/Y9WqVRERsdtuu8VPf/rTcvfXokWL+O///u9NnHXFtpbrcFXxt7/9Ld5///2IWPexfeuvf5Rn5513zl1DKioqiscee6zG5sK2rV5tTwAAAKCmDBgwoMLb99tvv2qN79SpU2571qxZG9w+f/78mDx5ckRE7LPPPtGlS5dK59ioUaPo3bt3vPDCC/Gvf/0rpk6dWuEFmC39mMpy+umnV2nMxIkTIyJi+PDhlY4/+eSTq3Tsr3vrrbdi0qRJMXv27FiyZEmsXr261O15eXkREfH2229HcXFx1KlT8XuE/uu//qvC2/Py8qJLly7x6aefRkTE7NmzNzjnf//733Pbp5xySm4OFenfv39ue9SoUZWOL0/btm1z23fddddGhUtLly6NCRMm5L4uGQyV54c//GHuWOPHj4/ly5dHkyZNqn3skjb3eWzUqFEcf/zxFY4pGYnNnj270uNvTp9//nmMGTMmpk2bFosXL47ly5dHlmW520v+zCZPnrzB87IqSj5/nnrqqbjqqqvKDUgBAABga7Gx15WqorJrRU2bNo327dvH9OnTI8uy+Oijjzb4N/mIESNy29/73vdycVV5BgwYEOedd14usKppW8t1uKooeX3o1FNPrdJ9vn596NJLL93kebDtE0sBAADbjJIhUFm222673Hbz5s2jdevWFY7ffvvtc9tlvUvsjTfeyG2vXLkyLrzwwirNc+bMmbntOXPmVBhLVecxRUSFK2tFVP6Yvm6HHXaIPffcs9JxvXv3zm1Pnjw5siyrMHZZvxJVVQ0bNixuuOGGmDFjRpXGFxUVxb/+9a8Nzs/XVSUwadmyZW67sufB8OHD46OPPqp0nyWjlzlz5lQ6vjwDBw6M+++/PyLWxVITJ06MM844I44++ugq/dwi1l34Wrt2bUREFBQUVPh8XK9r167RpEmTWL58eaxduzamTJkSBx100EY/jojNfx733nvvqF+/foVjKvtZbwnvvvtuXHHFFfHCCy/kfi6VWbhw4UYdq1evXtG2bduYM2dOfPzxx7HvvvvGoEGD4vjjj4+ePXtGgwYNNmq/AAAAsDlV97pSddTEtaL1b66MiOjZs2el+2vcuHF06tSp1BujatLWch2uKkpeH3ryySdj5MiRld7nX//6V257U66zkRaxFAAAsM1o3rx5hbeXfBdXZWO/Pr6oqGiD2+fNm5fbnjVrVtxxxx1VmWYpixcvrvD26jym6o4v6zF93a677lrpmK+PW716dSxdujSaNWtW7viqfuRXlmVx9tlnxwMPPFCl8SUtXbq00os0VXkelAxsKnsevPDCC9WY4TqVPQcqcvTRR8dFF10Ut99+e0SsW+Vp/PjxERGx0047xSGHHBL9+vWL7373u9GmTZsy97FgwYLcdtu2bau0olOdOnWibdu28d5770XExsc6JW3u81jdn/WaNWuqPYdN9eKLL8Z3vvOdDd6pWZmlS5du1PHq168fDz74YBx33HGxbNmyWLhwYdx8881x8803R6NGjeLAAw+MPn36xLe+9a046KCDqvTcAAAAgM1tc36UfE1cK/r6tZaqaNOmzWaLpbaW63BVUfL60KOPPlrt+2/KdTbSsunroAEAAGwlqvNCfk286F/yXUsbq7Igo7rzrOmYoXHjxlUa9/WPYKss3sjPz6/Sfu+5555SF2iOOeaYGDZsWLz99tuxePHiWL16dWRZlvuz22675cYWFxdXuv+t4XlQ1dWDyvPHP/4xnnrqqejRo0ep73/22Wfx5JNPxkUXXRS77rprDBgwID7++OMN7r9s2bLcdnU+Sq/k2I2NdUra3Odxaw99FixYEN/73vdyodRuu+0WN954Y4waNSrmzZsXK1asiOLi4txz/Ze//GXuvlV5rpenb9++MWXKlDj99NNL/V6uWrUqRo0aFTfccEMccsgh0bFjx3jmmWc2+jgAAABQU6p6XWlj1MT1g5LXWqp6ba2goGCTj1uereU6XFVs6vWh2njzG99MVpYCAADYSCVjkRNOOCGeffbZWpzN5rFixYoqjVu+fHmpr5s2bVojx7/lllty29ddd11ce+21FY6viWinuko+D5566qk48cQTt/gcTjzxxDjxxBPj448/jhEjRsTrr78er732Wrz77rsRse6dgU8++WTutr322it335IX477+c6xIybE18fPeGs5jbbrnnntyFwS7dOkSr776aoWrs9Xkc71du3YxbNiw+NOf/hSjRo2KUaNGxejRo2PMmDGxcuXKiIiYMWNGnHjiiXHrrbfGpZdeWmPHBgAAgG1NQUFB7t/4G3ttrTZsDdfhmjRpkjt3b775ZnTr1q3GjwERVpYCAADYaDvttFNu+9NPP63FmWw+c+bMqfa4hg0b1kg8M2fOnHj//fcjIqJFixZx1VVXVTh+yZIltbLU9tb0PNh1113j9NNPj7vuuiveeeed+Pjjj+O6667LvYtx0aJFG4QuJZdinzt3bmRZVulxiouLS/3Md9hhh02e+9Z0HmvDyy+/nNv++c9/XmEoFRHx0Ucf1fgcmjRpEkcffXRcf/318corr8SiRYvi8ccfj/322y835qqrropPPvmkxo8NAAAA24qS10nmzp1bpftUddzmsrVch0v9+hBbjlgKAABgI/Xs2TO3PXny5K3iHWA1bcGCBTFz5sxKx73xxhu57a5du9bIkuXz5s3LbXfs2DHq169f4fhRo0ZVKfSpaSWfB6NHj97ix69I27Zt49prr40hQ4bkvvePf/wj91FvERGdO3eOunXrRsS6dwS+/fbble53ypQpued73bp1o0uXLhuMqe5zYGs+j1tCyed7yTipLGvXrt0i5yg/Pz8GDBgQI0aMyF2s/Oqrr+LFF1/c7McGAACAb6quXbvmtseOHVvp+JUrV8bUqVM344wqt7muw7k+xNZKLAUAALCR2rVrF4WFhRGxLiC47777anlGm8eDDz5YrTGHHXZYjRy3Tp1//5O1KkuW33nnnTVy3Oo67rjjcttPPfVUfPbZZ7Uyj4qccMIJue2ioqL44osvcl83bdo0DjzwwNzXQ4cOrXR/JZ/rPXr0KPUReus1atSo1DEr8004j+tV97FVRXWe788888wWfXfl9ttvHwcffHDu6635ZwMAAAC1rV+/frntxx57LNasWVPh+CeffDJWrly5mWdVsc11HW5Trg/df//9sWrVqiodB6pLLAUAALAJrrjiitz2z3/+8yqtyrPeN2Up6dtuuy1mzZpV7u1Dhw6N8ePHR8S6d4udffbZNXLcPfbYI/fus6lTp8aHH35Y7thHH300nnvuuRo5bnX16NEjdxFs5cqV8YMf/CC++uqrKt33q6++2qQlyxcuXFilcSU/Mq9OnTrRsmXLUrefd955ue077rgj3nrrrXL3NXHixLj77rtzX59//vlljit5jKp8bFttnsfqqu5jq4p27drltv/617+WO27BggVxySWX1MgxFy1aVOWxJZ9DO+64Y40cHwAAALZFp556ai4SmjVrVvzud78rd+y//vWv+MUvfrGlplauzXUdrrrXUE466aTYc889IyJi/vz58aMf/ajKK8kvW7Zsm1z5n81DLAUAALAJvv/970f//v0jYt1HmB1yyCFx9913lxt5LFmyJP7yl79Ev3794qKLLtqSU90oDRo0iKVLl8aRRx4Zb7755ga3P/DAA6VCm7PPPjt3QWNT7bDDDtGrV6+IiCguLo4BAwbE9OnTS40pLi6OO+64I37wgx9E3bp1S71bbUu6/fbbo6CgICIi/vnPf0afPn0qXGZ9xowZcf3118fuu+++SUuK9+7dO0499dR44YUXyn3OzZgxI84444zc14cffng0aNCg1JjTTjst91F6X331VRx99NExfPjwDfb10ksvxbHHHpt7R+T+++8fp5xySpnH7dSpU2772WefrVL4VFvnsbpKPrbHH3+8RvZ5/PHH57ZvvPHGeOihhzYY8+abb0bfvn1jzpw5Za7mVV233357dO3aNe68885y481ly5bFNddckwsi69atG0cdddQmHxsAAAC2Vdtvv31ceumlua+vvPLK+P3vfx/FxcWlxs2ePTuOOeaYmD17djRs2HBLT7OUzXUdruQ1lCeeeKLS8Klu3bpx5513Rt26dSNi3bXHb3/72zFt2rRy7zN58uS44oorom3bthW+4RNKqlfbEwAAAPgmq1u3bjz22GNx5JFHxqRJk2LJkiVx/vnnx+WXXx69e/eO1q1bR926dWPx4sUxffr0mDZtWi40Oemkk2p59pXr3bt3bL/99vH000/HgQceGL169YrCwsJYvXp1vPHGG6XeZVZYWBi33HJLjR7/+uuvj6OOOiqKi4tj0qRJsd9++8XBBx8c7dq1i2XLlsVrr70W8+fPj4iIX//61zFkyJD46KOPanQOVdGpU6d4+OGH43vf+16sWLEixo4dG7169Yr27dvH/vvvH9tvv32sWrUqPv/883jrrbdqbDWioqKiePjhh+Phhx+O/Pz86Ny5c7Rr1y6aNWsWixcvjg8//DAmTJiQG5+fn1/mz6hBgwbx8MMPR9++fWPBggXx6aefRv/+/aNLly7RtWvXiFh34WnKlCm5++y4447x8MMPR/369cuc27HHHhv5+fmxcuXKmDx5chQWFka/fv2iRYsWuXcqHnXUUaXCm9o6j9V10kknxYsvvhgR61aXe+GFF2LfffctdWHzmmuuie22267K+zzjjDPi1ltvjRkzZsTq1avjBz/4Qdxwww3RpUuXaNSoUUydOjX3s+zSpUscffTR8dvf/naTH8uUKVPiRz/6UVxwwQXRvn376NSpU+ywww5RVFQU8+fPj9dffz2WLVuWG3/llVdG27ZtN/m4AAAAsC279tpr46WXXopx48ZFcXFxXHLJJXHLLbfEoYceGgUFBfHhhx/Gq6++GmvWrInevXtHu3bt4i9/+UtElP5IvC1pc1yH+8///M+4+uqrI8uyeP7556Nz585x0EEHRdOmTXNjTj755DjwwANzXx9xxBFx5513xuDBg2Pt2rXxwgsvxP/9v/839tlnn+jcuXM0a9YsVqxYEfPnz48pU6bEggULNs8JYZsmlgIAANhELVu2jNGjR8ell14a9957b6xZsyaWLFmSiynKkp+fHwcccMAWnOXGGzp0aBQVFcVzzz0Xb7zxRrzxxhsbjOnZs2c888wz0bx58xo99uGHHx533HFHXHTRRbFmzZooKiqKESNGxIgRI3Jj6tSpEz//+c/jqquuiiFDhtTo8avjuOOOi9dffz3OPvvsmDhxYkREzJw5M2bOnFnufXbfffdo06bNRh+z5IWllStXxtixY8tdiWmPPfaIhx56KDp37lzm7YWFhTFq1Kg4+eSTY9KkSRGxLqQpGUitt//++8djjz0W7du3L3duzZs3j9tuuy23XPqHH364wRLuBQUFG6xSVBvnsbrOPPPMeOihh+LVV1+NLMti+PDhG6zEdeGFF1YrlmrYsGH87W9/i2OPPTZ3nqZNm7bBOycPPvjgePTRR+Oee+7Z5MdR8vmTZVl88MEH8cEHH5Q5tkGDBnHNNdfEtddeu8nHBQAAgG1dw4YN48UXX4yTTjopXnnllYhY9zF0jzzySKlxBx10UDz55JNx2WWX5b7XrFmzLTrX9TbHdbi99torrrzyyrjxxhsjYt1H/E2dOrXUmE6dOpWKpSIizjnnnNhzzz3jvPPOi/fffz+yLIt33nkn3nnnnXKPte+++8b2229fjUdMysRSAAAANSA/Pz/uvPPOuOKKK+Khhx6KV155JWbMmBGLFi2K4uLiaN68ebRr1y66dOkShx9+eBxzzDG1duGjupo1axZ//etf44knnohhw4bFW2+9FZ999lm0aNEiOnfuHKeddlqcfvrpm+1db+eff34cfPDB8bvf/S6GDx8e8+bNi/z8/GjdunX0798/zjrrrOjWrdtmOXZ1denSJSZMmBD/+Mc/4plnnonRo0fHvHnz4ssvv4yGDRtGq1atYu+9946ePXvG0UcfHb17986tsrQxJk+eHGPGjInhw4fHuHHjYvr06TFv3rxYsWJFNG7cOHbeeefo2rVrnHDCCTFw4MBKl3Tfa6+9YsKECfHEE0/Ek08+GePGjYvPP/88ItatJNWzZ88YMGBAnHTSSVWa9/nnnx/77bdf3H333TF27Nj45JNPYsWKFZUuub6lz2N11a9fP1566aW477774sknn4ypU6fGF198UaWPGqzIXnvtFZMmTYo77rgjnnrqqZg+fXp89dVXsfPOO8d+++0Xp556agwcODC3FP2muuyyy+Kkk06Kf/7zn/H666/H22+/HbNnz44lS5ZEnTp1okWLFlFYWBj9+/eP008/PXbbbbcaOS4AAACkoEWLFvHyyy/HY489Fn/+859j4sSJ8cUXX8QOO+wQhYWF8YMf/CBOPfXUqF+/fnzxxRel7ldbNsd1uBtuuCEOOeSQeOCBB2LixInx2WefxYoVKyq932GHHRbTpk2LZ555Jp5//vkYM2ZMfPrpp7FkyZJo3Lhx7LTTTtGxY8c46KCD4thjj82tjg5VkZdVdoUSAAAAAAAAAIDNonXr1jFv3ryIiPj0009jp512quUZwbatdj7sEgAAAAAAAAAgcaNGjcqFUm3bthVKwRYglgIAAAAAAAAA2MK++uqruOSSS3Jfn3rqqbU4G0iHWAoAAAAAAAAAoAYNHjw47r///li6dGmZt0+dOjX69+8fEyZMiIiIgoKC+NGPfrQlpwjJysuyLKvtSQAAAAAAAAAAbCv69esXI0eOjIYNG0bXrl2jQ4cOUVBQEEuWLIm33nor3nnnnVifa+Tl5cV9990XgwYNquVZQxrq1fYEAAAAAAAAAAC2RatXr46xY8fG2LFjy7y9RYsWcccdd/gIPtiCrCwFAAAAAAAAAFCDPv3003j66adj5MiRMX369Fi4cGEsWrQoIiJatmwZnTp1iiOPPDLOOuusaNGiRe1OFhIjlgIAAAAAAAAAAJJQp7YnAAAAAAAAAAAAsCWIpQAAAAAAAAAAgCSIpQAAAAAAAAAAgCSIpQAAAAAAAAAAgCSIpQAAAAAAAAAAgCSIpQAAAAAAAAAAgCSIpQAAAAAAAAAAgCTUq+0JpGrVqlXx9ttvR0REq1atol49PwoAAAAAAADY1qxZsyYWLFgQERH77bdfNGrUqJZnVHu8RgpAdW2Ov0f97VNL3n777ejRo0dtTwMAAAAAAADYQsaNGxfdu3ev7WnUGq+RArApaurvUR/DBwAAAAAAAAAAJMHKUrWkVatWue1x48bFLrvsUouzAQAAAAAAADaH+fPn51ZTKvkaYYpKP/68//8HACqS/f8/Nff3qFiqlpT8/N1ddtkl2rRpU4uzAQAAAAAAADa3kq8Rpqj04xdLAVBV62Kpmvp71MfwAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAAAASRBLAQAAAAAAAABUYNddd41bbrk5pk2bGsuW/SsWLfo8xo17I37608siPz+/xo5zzDHHxFNPPRFz5syOVauWx5w5s+Opp56IY445psr7qFu3bpx33rnx6qsj4vPP58eKFUvjgw+mx113/Sn22WefGpsrfFPlZVmW1fYkUjR37txo27ZtRETMmTMn2rRpU8szAgAAAAAAAGqa1wX/reS5WLeuR15tTgeq7LjjjouHHhoWzZs3L/P26dOnx7e/fULMnDlzo4+Rl5cXQ4bcFT/84dnljrnnnnvjvPMGR0WZR8uWLePvf38uevToXubtq1atigsv/HHcd9/9Gz1X2LKyiCiOiJr7e9TKUgAAAAAAAAAAZejatWs8+uj/RvPmzWPp0qVx9dU/j969D4n+/Y+MIUPuiYiIvffeO55//q9RUFCw0cf59a//Ty6UevPNN+Pkk0+N7t17xcknnxpvvvlmREScc84P4//8n+vL3UedOnXi6aefzIVSTz75VBxzzLejR4/ecdFFF8dnn30WjRo1irvvvrNaK1XBtsbKUhHx0UcfxR//+Md4/vnnY86cOdGwYcNo3759DBw4MC644IJo3LhxjR9TQQ4AAAAAAADbvq31dcHafo3UylJ8U4wcOTz69Dk0ioqKok+fw2LMmDGlbv/pTy+Lm2++KSIifvWr/47rrvvvah+jQ4cO8c47b0X9+vVj/Pjx0afPYbFq1arc7fn5+TFy5CvRvXv3KCoqisLCTmWuYjVo0Jlx//33RkTEHXf8KS688Melbm/fvn1MnDgumjdvHu+//34UFnaKtWvXVnu+sGVZWarG/e1vf4vOnTvHbbfdFtOnT48VK1bE4sWLY8KECXH55ZdHt27d4oMPPqjtaQIAAAAAAADUCK+RQtV07949+vQ5NCIi7rvv/g1CqYiIW2+9Ld59992IiLj44ouiXr161T7OT37y46hfv35ERFx00U9KhVIREStXroyLLvpJRETUr18/Lrnk4jL389OfXhoREYsWLYqf/eyKDW6fOXNm3HjjurCrQ4cOceKJ3632XGFbkHQsNWnSpPje974XS5YsiYKCgvj1r38dr7/+erz88stxzjnnRETEjBkz4tvf/nYsXbq0lmcLAAAAAAAAsGm8RgpV993vfie3/cADw8ock2VZ/PnPD0VExHbbbReHHXZYtY/zne+cEBER06ZNi7Fjx5Y5ZuzYsfHee++VGl9Shw4dYp999omIiMceezxWrlxZ5n6GDv334xBLkaqkY6mLL744Vq5cGfXq1Yt//OMfcfXVV0fv3r2jf//+MWTIkPjtb38bEev+Z+DWW2+t5dkCAAAAAAAAbBqvkULVHXLIwRERsWzZspg4cWK540aOfDW3ffDBB1XrGHvssUe0bt16g/1UdJw2bdrE7rvvXuZcK9vPZ599FtOnT9+oucK2ItlYaty4cfHaa69FRMTZZ58dvXv33mDMZZddFoWFhRER8Yc//CGKioq26BwBAAAAAAAAaorXSKF6Cgs7RkTEBx98EGvXri133PoVn0rep6rWrwa1bj/TKxxb8vb1v6ebsp+2bdtG48aNqzxX2FYkG0s988wzue1BgwaVOaZOnTpx+umnR0TEl19+GcOHD98SUwMAAAAAAACocV4jhapr2LBhtGrVKiIi5s79pMKxX375ZSxbtiwi1gVI1dGmTevc9ty5cyscO2fOnNx227ZtNnk/derUiTZt2lQ4FrZFycZSo0aNioiIJk2axAEHHFDuuL59++a2R48evdnnBQAAAAAAALA5eI0Uqq5p06a57fUhVEWWL18eEREFBU024TjLKznGitx2QUFBBfupeL4V7QdSkGwsNW3atIiI2HPPPaNevXrljuvY8d9L5K2/DwAAAAAAAMA3jddIoeoaNWqU2/7qq68qHb969eqIiMjPz99sx1l/jLKOU1P7gRSU/zfgNmzVqlWxcOHCiIhKl5TbbrvtokmTJrF8+fJSS9pVprJl7ebPn1/lfQEAAAAAAABsCq+RQvWsWrUqt92gQYNKxzds2DAiIlauXLnZjrP+GGUd5+v7KRlEVWc/kIIkY6mlS5fmtquypNz6/xGoytJ661X3c0gBAAAAAAAANhevkUL1bMzvTETlH6VX8XEq/gi/Jk0a57a//rv59flWFEtVtB9IQZIfw7elClAAAAAAAACArYHXSKF6Vq9eXWI1ttYVjm3RokUuqKrOamwREXPnfpLbrmzVt5JB4pw5pVdy25j9FBcXV7oiHGyLklxZakt8tmhl/wGcP39+9OjRo8r7AwAAAAAAANhYXiOF6nv33WnRp8+hseeee0bdunVj7dq1ZY7r2LFjbnvatPeqeYx3S+xn7wrHlrx92rRpFe5nypQple5nzpw5sWLFimrNF7YFScZSTZs2zW1XZUm55cvXLZNXlaX11qus1AQAAAAAAADYUrxGCtU3atTo6NPn0CgoKIgDDjggxo0bV+a4vn375LZHj369WseYNWtWfPLJJ9G6detS+ylLnz6HRkTE3LlzY/bs2RvMteR8Hn30sTL3sdNOO8Xee++9UXOFbUWSH8PXqFGjaNmyZUREpUvKLV68OPc/Aj5jFwAAAAAAAPgm8hopVN8zzzyb2x406Iwyx+Tl5cXpp38/Itb97gwfPrzax3n22b9GRERhYWH07NmzzDE9e/aMwsLCUuNLev/993OrSw0c+F/lrgp35pn/fhxPP/1MtecK24IkY6mIiH322SciIj744INYs2ZNuePee+/fS+St/w8PAAAAAAAAwDeN10ihesaPHx+vvvpaREScffZZ0atXrw3GXHbZpbnfrT/84fYNfrf69u0bWbYmsmxNPPDAfWUe5/e//2Pufrff/vtSH5sZsS52vP3230dERFFRUfz+938scz+33HJbRES0bNkyfvvb32xwe7t27eKqq66IiHVxlViKVCUbSx1yyCERsW75yIkTJ5Y7buTIkbntgw8+eLPPCwAAAAAAAGBz8BopVN/FF18SK1asiPr168c//vFCXHnlFdGzZ8/o169f3HXXn+Lmm2+KiIjp06fHrbfetlHHeP/99+Pmm2+NiIju3bvH6NGvxsCB/xUHHHBADBz4XzF69KvRvXv3iIi4+eZb44MPPihzP8OG/Tn3cXwXXnhBPP74o3HUUUdF9+7d44ILfhSvv/5aNG/ePNauXRs//vElsXbt2o2aL3zT5WVZltX2JGrDuHHjcsvXnXfeeXHXXXdtMKa4uDg6deoU06ZNixYtWsTnn38e9evXr5Hjz507N7dk5Zw5c3x+LwAAAAAAAGyDtqbXBbem10jXreuRVyP7hc3tuOOOi4ceGhbNmzcv8/bp06fHt799QsycOXOD2/r27RsjRrwcERFDhw6LQYPOLnMfeXl5cc89d8fZZ59V7jzuvfe+OPfc86OizKNly5bx978/Fz16dC/z9lWrVsWFF/447rvv/nL3AVuXLCKKI6Lm/h5NdmWpHj16xKGHHhoREffdd1+88cYbG4y59dZbY9q0aRERcfHFF9fY/wQAAAAAAAAAbGleI4WN89xzz0Xnzt3ittt+H9OnT4/ly5fH4sWLY/z48XH55VdGt24HlhlKVUeWZfHDH54b3/rW8fHMM8/GJ598EqtXr45PPvkknnnm2Tj22OPinHPOqzCUiohYtGhRHHTQITF48AXx2mujYuHChbFy5cqYOXNmDBlyTxxwQA+hFMlLdmWpiIhJkybFwQcfHCtXroyCgoK4+uqr47DDDouVK1fGI488EkOGDImIiL322ismTJgQTZs2rbFjb00FOQAAAAAAALB5bG2vC24tr5FaWQqAqqn5laXqbfIevsG6desWjz76aHz/+9+PJUuWxNVXX73BmL322iuef/75Gv2fAAAAAAAAAIDa4DVSAFKX7MfwrXf88cfHW2+9FZdccknstdde0bhx42jRokUceOCBcdNNN8WkSZNizz33rO1pAgAAAAAAANQIr5ECkLKkP4avNm1ty20CAAAAAAAANc/rgv/mY/gAqL6a/xi+5FeWAgAAAAAAAAAA0iCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAgAAAAAAAAAAkiCWAoD/x969R3td1/kef/02G/CGBWaICtWM4iW8ICJjKpplpeYtCy9gplbOScdzbMxDaWo6No6Ns84ajUrUIzgzeU/BW02KCl7ZaopHvOAlxBpAEUFRYW++549yzzBcRGR/t+7P47EWa333/n2+n897t1ar1vo91/cLAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBHEUgAAAAAAAAAAQBFqiaXefvvtNb73scceW4uTAAAAAAAAAAAApaolltppp53y6KOPvuf7fvKTn2TYsGEdMBEAAAAAAAAAAFCaWmKp6dOnZ9iwYfnJT36yWutnzZqVvffeO6NHj87ixYs7eDoAAAAAAAAAAKAEtcRSH/nIR7J48eKMHj06e++9d2bNmrXStb/85S+z/fbb56677kpVVdl9993rGBEAAAAAAAAAAOjiaomlHn300QwfPjxVVeWuu+7K9ttvnyuvvHKZNQsWLMjIkSMzatSozJ8/P83Nzfm7v/u7TJo0qY4RAQAAAAAAAACALq6WWGrAgAGZNGlSfvzjH6e5uTnz58/PyJEjc9RRR2XBggW588472wOqqqqy1VZb5d57780PfvCDNDXVMiIAAAAAAAAAANDF1VYiNRqNjB49Ovfee28GDhyYqqryb//2bxk4cGA+//nPZ+bMmamqKn/913+dhx9+OEOGDKlrNAAAAAAAAAAAoAC1P7ZpyJAheeSRR3LQQQelqqrMnTs3S5cuzYYbbpibbropY8aMybrrrlv3WAAAAAAAAAAAQBfXKe+4Gz9+fH7zm9+k0WikqqokycKFCzNhwoS8+eabnTESAAAAAAAAAADQxdUaS7388ss56KCD8p3vfCdvvvlm1l133Zx55pn55Cc/maqqMnbs2Oy000556KGH6hwLAAAAAAAAAAAoQG2x1K233prtttsuN910U6qqypAhQ/Lwww/nzDPPzKOPPppRo0alqqo8/fTT+cxnPpNzzz23/alTAAAAAAAAAAAA71ctsdSJJ56YL3/5y5k9e3YajUa+//3v57777svAgQOTJL169cr48eNz1VVX5aMf/WiWLFmSM844I8OHD88LL7xQx4gAAAAAAAAAAEAXV0ssNWbMmFRVlQEDBmTSpEk599xz09zcvNy6r33ta5k2bVr23nvvVFWVe+65JzvuuGMdIwIAAAAAAAAAAF1cba/hGzlyZB577LHsscceq1y36aab5re//W3+8R//MT179szChQtrmhAAAAAAAAAAAOjKaoml/vVf/zVXXHFFNtxww9W+57vf/W4efPDBDBo0qAMnAwAAAAAAAAAASlFLLHXEEUes0X3bbbddpk6dupanAQAAAAAAAAAASlTba/jWVI8ePTp7BAAAAAAAAAAAoAuoPZa6/fbbc9RRR2WLLbbIBhtskObm5jzxxBPLrLn77rszZsyY/Mu//Evd4wEAAAAAAAAAAF1Uc10HLVq0KEcffXSuv/76JElVVUmSRqOx3Npu3brlxBNPTKPRyLBhw7LlllvWNSYAAAAAAAAAANBF1fZkqREjRuT6669PVVUZOnRoTjnllJWu3W233TJo0KAkyXXXXVfXiAAAAAAAAAAAQBdWSyx13XXX5ZZbbkmSXHzxxbn//vtz/vnnr/Ker3zlK6mqKnfddVcdIwIAAAAAAAAAAF1cLbHUuHHjkiSjRo3KN7/5zdW6Z8iQIUmS6dOnd9hcAAAAAAAAAABAOWqJpVpaWtJoNHLYYYet9j39+vVLksydO7ejxgIAAAAAAAAAAApSSyz1yiuvJEk23XTT1b6nqelPoy1durRDZgIAAAAAAAAAAMpSSyz1kY98JEnyhz/8YbXvef7555MkH/vYxzpkJgAAAAAAAAAAoCy1xFIDBw5Mkjz66KOrfc8NN9yQJBk8eHBHjAQAAAAAAAAAABSmllhq//33T1VVufDCC/PWW2+96/rJkyfnyiuvTKPRyAEHHFDDhAAAAAAAAAAAQFdXSyx1wgknpE+fPpk9e3a++tWvZt68eStc19ramrFjx+bLX/5yli5dmv79++cb3/hGHSMCAAAAAAAAAABdXHMdh2y44Ya56qqrst9+++XWW29N//79s+eee7Z/fuqpp2bx4sVpaWnJa6+9lqqqss466+Tqq69O9+7d6xgRAAAAAAAAAADo4mp5slSSfO5zn8sdd9yRAQMG5M0338xtt92WRqORJLn11ltz++23Z/78+amqKv3798+kSZOyyy671DUeAAAAAAAAAADQxdXyZKl37LbbbnnmmWdy5ZVXZsKECWlpacmcOXPS1taWjTbaKIMHD86BBx6Yo48+Oj169KhzNAAAAAAAAAAAoIurNZZKkubm5owaNSqjRo2q+2gAAAAAAAAAAKBgtb2GDwAAAAAAAAAAoDOJpQAAAAAAAAAAgCKIpQAAAAAAAAAAgCI0r83NunXrtja3S5I0Go20trau9X0BAAAAAAAAAICyrNVYqqqqtbkdAAAAAAAAAADAWrNWY6kzzzxzlZ/ffPPNaWlpSZJ8+tOfzi677JK+ffsmSWbPnp2pU6fm8ccfT6PRyM4775z99ttvbY4HAAAAAAAAAAAUrLZY6uyzz05LS0t22GGHXHzxxRk6dOgK102dOjXHH398Wlpasv/+++eMM85YmyMCAAAAAAAAAACFaqrjkNtvvz1nnXVWBg4cmClTpqw0lEqSoUOHZvLkydliiy3yox/9KL/97W/rGBEAAAAAAAAAAOjiaoml/vmf/zmNRiOjR4/O+uuv/67r119//YwePTpVVeXCCy+sYUIAAAAAAAAAAKCrqyWWamlpSZJsv/32q33PDjvskORPr+UDAAAAAAAAAAB4v2qJpebNm5ckee2111b7ngULFiRJXn311Q6ZCQAAAAAAAAAAKEstsdSmm26aJLnuuutW+55rr702SdKvX78OmQkAAAAAAAAAAChLLbHUl770pVRVlV/84he5+uqr33X9tddem1/84hdpNBrZb7/9apgQAAAAAAAAAADo6mqJpX7wgx9kww03zNKlS3PEEUfk4IMPzg033JCXXnopS5YsSWtra1566aXccMMNOeSQQ3LYYYelra0tvXr1yve///06RgQAAAAAAAAAALq45joO2WyzzTJx4sQccMABWbBgQSZOnJiJEyeudH1VVenVq1duvPHGbLbZZnWMCAAAAAAAAAAAdHG1PFkqSfbYY49MmzYthx56aJqamlJV1Qr/NTU15Stf+Uoee+yx7LnnnnWNBwAAAAAAAAAAdHG1PFnqHf37988111yT2bNnZ9KkSZk2bVrmzZuXJOndu3e22267fPazn80mm2xS51gAAAAAAAAAAEABao2l3tG3b98cfvjhOfzwwzvjeAAAAAAAAAAAoEC1vYYPAAAAAAAAAACgM4mlAAAAAAAAAACAItT+Gr5XXnkl9913X5577rksXLgwbW1t73rPGWecUcNkAAAAAAAAAABAV1ZbLDVnzpycfPLJufbaa9Pa2vqe7hVLAQAAAAAAAAAA71ctsdSrr76a3XffPc8++2yqqqrjSAAAAAAAAAAAgGU01XHIeeedlxkzZqSqqnzhC1/Ibbfdlrlz56atrS1Lly59138AAAAAAAAAAADvVy1PlrrxxhvTaDSy//77Z8KECXUcCQAAAAAAAAAAsIxaniw1c+bMJMkJJ5xQx3EAAAAAAAAAAADLqSWW2mCDDZIkffv2reM4AAAAAAAAAACA5dQSS2233XZJkt///vd1HAcAAAAAAAAAALCcWmKp448/PlVV5YorrqjjOAAAAAAAAAAAgOXUEkuNGDEiI0eOzK9+9aucd955dRwJAAAAAAAAAACwjOY6Drn77rtz3HHH5fnnn89pp52W66+/PkceeWS23nrrrLfeeu96//Dhw2uYEgAAAAAAAAAA6MpqiaX22muvNBqN9p8feuihPPTQQ6t1b6PRSGtra0eNBgAAAAAAAAAAFKKWWCpJqqqq6ygAAAAAAAAAAIDl1BJLTZo0qY5jAAAAAAAAAAAAVqqWWGrPPfes4xgAAAAAAAAAAICVaursAQAAAAAAAAAAAOoglgIAAAAAAAAAAIoglgIAAAAAAAAAAIrQvDY3O/bYY5MkjUYjl1566XK/XxP/fS8AAAAAAAAAAIA10aiqqlpbmzU1NaXRaCRJ2traVvj796KqqjQajWX26ipmzZqV/v37J0lefPHFbL755p08EQAAAAAAALC2+V7wP/3X/yz+9BKk9/4dMgClqZIsTbL2/nd0rT5ZasCAASuMolb2ewAAAAAAAAAAgLqs1VjqhRdeeE+/BwAAAAAAAAAAqEtTZw8AAAAAAAAAAABQB7EUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQBLEUAAAAAAAAAABQhObOOHThwoV5/vnns3DhwrS1tb3r+uHDh9cwFQAAAAAAAAAA0JXVGkuNHTs2Y8aMybRp01JV1Wrd02g00tra2sGTAQAAAAAAAAAAXV0tsVRbW1sOPfTQTJw4MUlWO5QCAAAAAAAAoOt5/oUrs/nmG3f2GAB8wM2aNTef+uSItbpnLbHUz3/+80yYMCFJ0rdv3xxzzDEZMmRI+vTpk6ampjpGAAAAAAAAAAAACldLLDV+/PgkybbbbpvJkyend+/edRz74fbGy8lP/nLZ333v2WT9j9nHPvaxj33sYx/7dL19Pkiz2Mc+9rGPfexjH/vYxz4l7PNBmsU+9rGPfexjn9L3AQBqVctjnaZPn55Go5Ef/vCHQikAAAAAAAAAAKBT1PoOvK222qrO4wAAAAAAAAAAANrVEkttueWWSZJ58+bVcRwAAAAAAAAAAMByaomlDj/88FRVlZtuuqmO4wAAAAAAAAAAAJZTSyx10kknZYcddsjPfvazTJ48uY4jAQAAAAAAAAAAllFLLNWzZ8/8+te/zpAhQ7LPPvvk1FNPze9+97u89dZbdRwPAAAAAAAAAACQ5joO6datW/t1VVW54IILcsEFF6zWvY1GI62trR01GgAAAAAAAAAAUIhaYqmqqlb5MwAAAAAAAAAAQEerJZY688wz6zgGAAAAAAAAAABgpcRSAAAAAAAAAABAEZo6ewAAAAAAAAAAAIA6iKUAAAAAAAAAAIAi1PIavv9uyZIlefjhh/P4449n3rx5SZI+ffpk0KBB2WmnndK9e/fOGAsAAAAAAAAAAOjCao2lFi1alHPOOSdjx47Nq6++usI1vXv3zre//e2cfvrpWW+99eocDwAAAAAAAAAA6MJqew3fzJkzs+OOO+b888/PvHnzUlXVCv/Nmzcv//AP/5DBgwdn1qxZdY0HAAAAAAAAAAB0cbU8WWrJkiXZd999M2PGjCTJ1ltvnWOOOSbDhg3LJptskiT5j//4jzz44IO5/PLL88QTT+SZZ57Jvvvum0ceeSTNzZ3ytkAAAAAAAAAAAKALqeXJUpdcckmmT5+eRqOR0047LdOmTcv3vve9DB8+PAMHDszAgQMzfPjwnHLKKXnsscdy+umnJ0meeOKJXHLJJXWMCAAAAAAAAAAAdHG1xFLXXHNNGo1GDj744Jxzzjnp1q3bygdqasrZZ5+dQw45JFVV5ZprrqljRAAAAAAAAAAAoIurJZZ6/PHHkyTHHnvsat9z3HHHJUmmTZvWITMBAAAAAAAAAABlqSWWeu2115Ikm2666Wrf069fvyTJggULOmQmAAAAAAAAAACgLLXEUn369EmSPP/886t9zztr37kXAAAAAAAAAADg/aglltppp51SVVV++tOfrvY9Y8aMSaPRyODBgztwMgAAAAAAAAAAoBS1xFJHHHFEkuTOO+/MsccemzfeeGOlaxctWpRvfvObueOOO5IkRx55ZB0jAgAAAAAAAAAAXVxzHYeMHDkyP//5z3Pvvfdm3LhxueWWWzJixIgMGzYsH//4x9NoNDJ79uw88MADufrqqzN37twkyW677ZaRI0fWMSIAAAAAAAAAANDF1RJLNRqNTJw4Mfvvv3/uv//+zJkzJz/96U9X+Fq+qqqSJLvuumtuvPHGOsYDAAAAAAAAAAAKUMtr+JKkd+/emTJlSi688MJss802qapqhf+22WabXHTRRZk8eXJ69+5d13gAAAAAAAAAAEAXV8uTpd7R1NSUE044ISeccEL++Mc/5vHHH8+8efOSJH369MmgQYPSr1+/OkcCAAAAAAAAAAAKUUssdeyxxyZJ9t1333zta19LkvTr108YBQAAAAAAAAAA1KaWWGrcuHFJksMOO6yO4wAAAAAAAAAAAJbTVMchG2+8cZKkb9++dRwHAAAAAAAAAACwnFpiqW233TZJ8vvf/76O4wAAAAAAAAAAAJZTSyw1atSoVFXV/jo+AAAAAAAAAACAutUSSx1zzDH53Oc+lxtvvDFnnXVWqqqq41gAAAAAAAAAAIB2zXUcMnny5JxyyimZO3duzjnnnFx11VU57LDDsv3226d3797p1q3bKu8fPnx4HWMCAAAAAAAAAABdWC2x1F577ZVGo9H+89NPP51zzjlnte5tNBppbW3tqNEAAAAAAAAAAIBC1BJLJfHqPQAAAAAAAAAAoFPVEktNmjSpjmMAAAAAAAAAAABWqpZYas8996zjGAAAAAAAAAAAgJVq6uwBAAAAAAAAAAAA6iCWAgAAAAAAAAAAiiCWAgAAAAAAAAAAitBcxyF77733Gt/baDRy++23r8VpAAAAAAAAAACAEtUSS915551pNBqpqmqlaxqNxjI/v7P2v/8eAAAAAAAAAABgTdQSSw0fPvxdo6c33ngjM2bMyPz589NoNDJw4MD069evjvEAAAAAAAAAAIAC1PZkqdV1yy235KSTTsq8efNy6aWXZrfdduu4wQAAAAAAAAAAgGI0dfYA/91+++2XKVOmpLm5OYccckheeumlzh4JAAAAAAAAAADoAj5wsVSSbLLJJjn55JPz8ssv5/zzz+/scQAAAAAAAAAAgC7gAxlLJcnuu++eJLn55ps7eRIAAAAAAAAAAKAr+MDGUj169EiS/OEPf+jkSQAAAAAAAAAAgK7gAxtLTZkyJUmy3nrrdfIkAAAAAAAAAABAV/CBjKXuu+++nH322Wk0Gtlll106exwAAAAAAAAAAKALaK7jkLPPPvtd1yxdujSvvvpqWlpa8sADD2Tp0qVpNBo5+eSTa5gQAAAAAAAAAADo6mqJpc4666w0Go3VXl9VVZqbm3P++ednn3326cDJAAAAAAAAAACAUtQSSyV/CqBWpdFopFevXvnUpz6VPffcM9/+9rez7bbb1jQdAAAAAAAAAADQ1dUSSy1durSOYwAAAAAAAAAAAFaqqbMHAAAAAAAAAAAAqINYCgAAAAAAAAAAKEKnxVJLly7Nyy+/nJkzZ6atra2zxgAAAAAAAAAAAApRayzV1taWSy+9NHvssUfWW2+99O3bN3/xF3+Rp556apl1N910U0499dSce+65dY4HAAAAAAAAAAB0Yc11HTRnzpwcfPDBeeCBB1JV1SrXfvKTn8yBBx6YRqOR/fffPzvuuGM9QwIAAAAAAAAAAF1WLU+WamtrywEHHJD7778/jUYjI0aMyEUXXbTS9YMGDcqwYcOSJL/61a/qGBEAAAAAAAAAAOjiaomlxo0bl6lTp6Z79+65+eabc+WVV+Y73/nOKu858MADU1VVpkyZUseIAAAAAAAAAABAF1dLLPXLX/4yjUYjxx9/fL74xS+u1j2DBw9Okjz11FMdORoAAAAAAAAAAFCIWmKpxx57LMmfnha1uj7+8Y8nSV555ZUOmQkAAAAAAAAAAChLLbHU/PnzkyQbbbTRat/T1taWJOnWrVtHjAQAAAAAAAAAABSmlliqT58+SZIXX3xxte955plnkiQbb7xxh8wEAAAAAAAAAACUpZZY6tOf/nSSZOrUqat9z1VXXZVGo5GhQ4d21FgAAAAAAAAAAEBBaomlDj744FRVlYsuuiivvvrqu66/9tprM3HixCTJoYce2tHjAQAAAAAAAAAABagllvrWt76VAQMGZMGCBfnCF76QJ554YoXr5syZk9NOOy1HHnlkGo1GBg0alBEjRtQxIgAAAAAAAAAA0MU113FIz549c+ONN2avvfbKQw89lO222y5bbbVV++ejRo3K66+/nueeey5VVaWqqmy00Ua57rrr0mg06hgRAAAAAAAAAADo4mp5slSS7LDDDpk6dWp23XXXVFWVJ598sv2zRx99NDNmzMjSpUtTVVV22WWXPPDAA9liiy3qGg8AAAAAAAAAAOjianmy1Du22GKL3HPPPZkyZUomTJiQlpaWzJkzJ21tbdloo40yePDgHHjggdlnn33qHAsAAAAAAAAAAChArbHUO3bffffsvvvunXE0AAAAAAAAAABQqNpewwcAAAAAAAAAANCZxFIAAAAAAAAAAEAROuU1fAsXLszzzz+fhQsXpq2t7V3XDx8+vIapAAAAAAAAAACArqzWWGrs2LEZM2ZMpk2blqqqVuueRqOR1tbWDp4MAAAAAAAAAADo6mqJpdra2nLooYdm4sSJSbLaoRQAAAAAAAAAAMDaUkss9fOf/zwTJkxIkvTt2zfHHHNMhgwZkj59+qSpqamOEQAAAAAAAAAAgMLVEkuNHz8+SbLttttm8uTJ6d27dx3HAgAAAAAAAAAAtKvlsU7Tp09Po9HID3/4Q6EUAAAAAAAAAADQKWp9B95WW21V53EAAAAAAAAAAADtaomlttxyyyTJvHnz6jgOAAAAAAAAAABgObXEUocffniqqspNN91Ux3EAAAAAAAAAAADLqSWWOumkk7LDDjvkZz/7WSZPnlzHkQAAAAAAAAAAAMuoJZbq2bNnfv3rX2fIkCHZZ599cuqpp+Z3v/td3nrrrTqOBwAAAAAAAAAASHMdh3Tr1q39uqqqXHDBBbngggtW695Go5HW1taOGg0AAAAAAAAAAChELbFUVVWr/BkAAAAAAAAAAKCj1RJLnXnmmXUcAwAAAAAAAAAAsFJiKQAAAAAAAAAAoAhNnT0AAAAAAAAAAABAHcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEcRSAAAAAAAAAABAEZo7ewAAAAAAAAAAgA+iOXNezdQHn8zUqdPT0vJUWlqezCuvLEiSHPX1L+ayy0av9TOvvPL2jLv8tkyb9mzmz389ffv2yW67b5f/8T8Ozq67fnq19li06K2M+emvcu11d+W5Z/+Qt99ekv79N86++/5VTvybr+QTn9hkrc8NHxZiKQAAAAAAAACAFdhs06/Udtabb76dw0acmVtvfWCZ38+cOTsz/212rrryjpx++tfzwzOOXuU+M2a8lAMPGJ1nnpm1zO+feurFPPXUi7nsslsyfvxp2f/Lu671vwE+DLyGDwAAAAAAAADgXQwY0Df77LNzh+3/rW+e3x5K7bXX4Fx3/Tm5976f5eKx38tf/uWmWbp0ac4++/KMHTtxpXssXLgoBx34/fZQ6rhv7p/f/OaC3D35opxzznHZYIN1s2DBGznyyLPzu9/N6LC/BT7Iin2y1Jw5c/Lggw/mwQcfzNSpUzN16tS88sorSZKjjz46l19+eecOCAAAAAAAALAW+Y4U3rvTT/96dt556+w8dKv07dsnL7zwH9lyiyPW+jmT7ng4V111R5Lky1/+TK697ux069YtSTJ06NY54IDdMmyX4zNz5uz84PsX56tf3Su9e/dabp8L/vHKPP30i0mS8847Pn97yuHtn+2666czfM8d87m9/1cWLXorf/vdi3L7Hf9nrf8t8EFXbCzVt2/fzh4BAAAAAAAAoDa+I4X37syzjqnlnH/6p6uTJM3N3XLhRf+rPZR6x8c+9pH8+O+/nVEjz8n8+a/nsktvXiaESpIlS1pz0UXXJ0m22eYTOfm7I5Y75zOfGZRjjt0vYy+emLvvfjRTpz6ZoUO37qC/Cj6YvIYvyYABA/KFL3yhs8cAAAAAAAAAqIXvSOGDY+HCRbnjjoeTJJ/73JBsvvnGK1x3yCF7ZMMN10+S3HDDlOU+v3PSI3nttTeSJEcd9cU0Na04Cfn617/Ufn3jDZPf1+zwYVTsk6XOOOOMDB06NEOHDk3fvn3zwgsv5FOf+lRnjwUAAAAAAADQIXxHCh9MLVOfzOLFS5Ikw4fvsNJ1PXp0z7Bh2+Tf/70lLS1PZsmS1nTv/p/Zxz33TGu/3mMV++y881ZZb711smjRW7n33sfXwl8AHy7FxlI/+tGPOnsEAAAAAAAAgNr4jhQ+mJ6Y/vv26622HrDKtVttPSD//u8taW1tyzPPzMq2236y/bPp/2WfrVexT3Nzt/zlFptm2mPP5cknZ6754PAh5TV8AAAAAAAAAACd5KVZc9uvN9tsxa/ge0f/zT/efj3rxTnLfDbrz/usv/46+ehHN1itfebOnZ+33178nuaFDzuxFAAAAAAAAABAJ1n4+qL26w02WHeVa9dbf53269ffeHOZz17/8z7vtkfyp6DqP+97cxUroesRSwEAAAAAAAAAdJK33vrPJzv16NG8yrU9e3Zvv37zzWWfCPXWW0v+vEf3vJseq9gHurpV/7eMNTZr1qxVfv7HP/6xpkkAAAAAAAAAOp7vSGHNrLNOj/brxYtbV7n27beXtF+vu26PZT5bZ53uf95jSd7N4lXsA12dWKqD9O/fv7NHAAAAAAAAAKiN70hhzfTaYL3263d7Jd6iN95qv95g/WVft7fBn/dZndfqvfFf91mN1/ZBV+I1fAAAAAAAAAAAnWSzzTduv37ppbmrXPvirDnt15v3//gyn23+533eeOOtzJ//+mrts/HGH03Pnp4sRVk8WaqDvPjii6v8/I9//GN22WWXmqYBAAAAAAAA6Fi+I4U1s+02n2i/furJmclBK1/71JMzkyTNzd2y5ZabL/PZNv9lnyefnJm/+qttV7hHa2tbnnv2D0mSrbcesKZjw4eWWKqDbL755u++CAAAAAAAAKCL8B0prJmdh26dHj26Z/HiJbn77kdz6v8+coXrFi9ekgcemP6ne3beOt27L5t87Lbbdu3Xk+9+dKWxVEvLU+2v4fvMZwatjT8BPlS8hg8AAAAAAAAAoJP06rVe9t57pyTJ7bc/lFmzVvwqvl/9anIWLHgjSXLwwbsv9/mee+2Yj3xk/STJFVf8OlVVrXCf8eNva78+6OA93tfs8GEklgIAAAAAAAAA6CDjxt2W7s2fTffmz+bsH12+wjXf/e6IJH96Rd5Jf/N/0tbWtsznL7/8Wn7w/YuTJB/96AY59rj9l9ujR4/uOfHEryRJpk//ff7pgquWW3Pfff8v//eyW5Ikw4fvkKFDt17jvws+rLyGDwAAAAAAAABgBaZMmZZnn32p/edXXn6t/frZGS9l3Ljblll/9NFfWqNzPrv3TjnssL1z1VV3ZOLEe/OlL34vJ/3PQ9Ov38fy+OPP5by//5fMnDk7SfLjv/92evfutcJ9/vaUw3PNNXfm6adfzOjRv8iMZ1/KYSP2zjrr9sxddz6S887717S2tmXddXvmgn86cY1mhQ87sRQAAAAAAAAAwApcdtnNuWL8r1f42b33Pp577318md+taSyVJGMvOTULFryRW299IHfe+UjuvPORZT5vamrKaacdlW9964CV7tGr13q5ccLf58ADRueZZ2blkrE35ZKxNy2zZsMN18/48adlxx23WONZ4cNMLAUAAAAAAAAA0MnWXbdnJkw8L7/85W8zftyv89hjz2b+/NfTt2/v7Lb7dvnOdw7Jrrt++l332WKLzTK15eL8bMwNufa6u/LsjJeyeHFr+vffOF/60rD8zUmH5hOf2KSGvwg+mIqNpaZMmZIZM2a0//zyyy+3X8+YMSOXX375Muu/8Y1v1DQZAAAAAAAAwNrnO1J47y67bHQuu2z0+9rj6KO/9J6eOHXEEZ/PEUd8/n2duf766+aU7x2RU753xPvaB7qiYmOpSy65JOPGjVvhZ/fcc0/uueeeZX7n/wgAAAAAAAAAH2a+IwWApKmzBwAAAAAAAAAAAKhDsbHU5ZdfnqqqVvsfAAAAAAAAwIeZ70gBoOBYCgAAAAAAAAAAKItYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCgAAAAAAAAAAKIJYCoD/3959h0tZ3W3DvrZUKYI0xaBgCRprVFRQjFgwKhaMJYKJoJL4+BhLojGxgvoaSx77G0uij1iiWNHELhaCoiJFgyYWFDWIIsVOh/n+4GU+tuy9KSJ7YM7zOPZx3Mxa95o1w49h9sx1rwUAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAWhKUAAAAAAAAAAICyICwFAAAAAAAAAACUBWEpAAAAAAAAAACgLAhLAQAAAAAAAAAAZUFYCgAAAAAAAAAAKAvCUgAAAAAAAAAAQFkQlgIAAAAAAAAAAMqCsBQAAAAAAAAAAFAW6tb2BMrV3Llzi8cfffTR4h2mT0u+mF/5tg8nJo1mLtsdGcc4xjGOcYxjHOOsCuOU0lyMYxzjGMc4xjGOcYxjnHIYp5TmYhzjGMc4xjHOaj7Oot8FLvodYTmq/B3p1FqcCQCrikX/v1hR/49WFAqFwgoZiWXy8ssvZ8cdd6ztaQAAAAAAAAAryYgRI7LDDjvU9jRqje9IAfg2VtT/o7bhAwAAAAAAAAAAyoKVpWrJzJkzM3bs2CRJ69atU7fu4jsifvTRR8Vk9YgRI9K2bduVOkdYkdQzqxP1zOpEPbO6UMusTtQzqxP1zOpEPbO6UMusTtQzq4q5c+dm8uTJSZKtttoqDRs2rOUZ1Z6l+Y4UABb1Xfw/6n+fWtKwYcNlWhqsbdu2adeu3Xc4I1h51DOrE/XM6kQ9s7pQy6xO1DOrE/XM6kQ9s7pQy6xO1DOlrkOHDrU9hZKwrN+RAkCy4v8ftQ0fAAAAAAAAAABQFoSlAAAAAAAAAACAsiAsBQAAAAAAAAAAlAVhKQAAAAAAAAAAoCwISwEAAAAAAAAAAGVBWAoAAAAAAAAAACgLwlIAAAAAAAAAAEBZqCgUCoXangQAAAAAAAAAAMB3zcpSAAAAAAAAAABAWRCWAgAAAAAAAAAAyoKwFAAAAAAAAAAAUBaEpQAAAAAAAAAAgLIgLAUAAAAAAAAAAJQFYSkAAAAAAAAAAKAsCEsBAAAAAAAAAABlQVgKAAAAAAAAAAAoC8JSAAAAAAAAAABAWRCWAgAAAAAAAAAAyoKwVIl6//33c+qpp2azzTZL48aN06JFi+ywww754x//mOnTp9f29Chzn3zySR566KGce+652XfffdOqVatUVFSkoqIiffv2XebxHn300Rx88MFp165dGjRokHbt2uXggw/Oo48+uuInD98wcuTInH/++dl7772LNdikSZN07NgxRx99dJ577rllGk89U1u++OKLDBo0KKeeemp22223bLLJJmnWrFnq16+fNm3apFu3brn00kszderUpRpv+PDh+dnPfpb27dunYcOGWXfddfPjH/84d95553f8SKBmv/vd74rvOyoqKvLss88u8RyvzdSmReu1pp9u3botcSy1TKn54IMP0r9//3Tq1CmtW7dOw4YNs/7662fXXXfNueeem9dee63G89U0taVbt25L/fq8NO851DKlYPbs2bnxxhvz4x//OG3bti1+vrHpppvm6KOPzvDhw5dqHPVMKZg5c2auvfba7LnnnmndunXq16+f9dZbL/vtt18GDRq01OP4bAMAgGoVKDl/+9vfCmuttVYhSZU/HTt2LLz99tu1PU3KWHW1maTQp0+fpR5n3rx5hWOPPbbG8fr161eYN2/ed/dgKGu77rprjfW38Oeoo44qzJo1q8ax1DO17cknn1yqem7VqlXhscceq3Gs/v37F9ZYY41qx+jRo0dhxowZK+mRwf9vzJgxhbp161aqx2eeeaba/l6bKQVL89qcpLDbbrtVO4ZaphRdffXVhcaNG9dYlyeffHKV56ppattuu+221K/PSQprrLFGYcKECYuNo5YpFe+9915hiy22WGItn3jiiYX58+dXOYZ6plS88cYbhU033bTGWtx7770LX375ZY3j+GwDAICaWFmqxIwZMyY//elP88UXX6RJkya58MILM3z48Dz11FP5xS9+kSR566230qNHj3z55Ze1PFtINthgg+y9997Lde5ZZ52Vm266KUmy7bbb5s4778yIESNy5513Ztttt02S3HjjjTn77LNX2HxhURMnTkySrLfeejn55JNz7733ZsSIEXnhhRdy+eWX53vf+16S5NZbb13iqmnqmVKw/vrr56ijjspVV12V+++/Py+88EKef/753HXXXTnssMNSp06dTJkyJQceeGBeffXVKse44YYbct5552X+/PnZeOONc9NNN2XEiBF54IEHsvvuuydJHn744RxzzDEr86FB5s+fn1/+8peZO3du2rRps1TneG2mlBx//PEZO3ZstT8333xzteeqZUrN//k//ycnnXRSvv7663Ts2DF//OMf8+yzz2bMmDEZMmRI/vjHP2bnnXfOGmtU/bGTmqa23XzzzTW+Jo8dOzZ33XVXsf+ee+5Z/P1wUWqZUjBnzpz06NEjr7/+epJk6623zsCBA/PCCy/kiSeeyLnnnpvGjRsnSa655ppccsklVY6jnikFn3zySbp3754333wzSXLYYYfloYceyujRo/PQQw/lsMMOS5I88cQTOeKII6odx2cbAAAsUW2ntahs4SondevWLQwfPnyx9ksvvbR45UP//v1X/gShUCice+65hb///e+Fjz/+uFAoFArjx48v1uXSriz15ptvFleG6NSpU2H69OmV2r/++utCp06div8erKbGd6FHjx6Fu+66qzB37twq2ydPnlzo2LFjsb6HDh1aZT/1TCmoro4XNXjw4GI9H3zwwYu1T506tdCsWbNCksIGG2xQmDx58mL3ccABByzVij6wol1xxRWFJIXNNtuscMYZZyyxDr02Uyq+7e9vaplSM2TIkGJdH3XUUYXZs2dX27eq1VnVNKuK008/vVjrt91222LtaplScc899xRrtUuXLlX+bjhy5MhCvXr1CkkKzZs3L8yZM6dSu3qmVJxwwglLfP987rnnFvvcc889i7X7bAMAgKVhZakSMmLEiAwbNixJcuyxx6ZLly6L9Tn11FPzgx/8IEly1VVXZc6cOSt1jpAk5513Xvbff/+ss846yz3GlVdemblz5yZZcFXbmmuuWam9UaNGueaaa5Ikc+fOzRVXXLH8E4ZqPPTQQzn88MNTp06dKttbtWqVyy67rPjne++9t8p+6plSUF0dL6pnz57ZdNNNk6T4nmNRN954Yz7//PMkySWXXJJWrVotdh/XXntt8b7++Mc/fttpw1L54IMPcs455yRJrr/++tSvX3+J53htZnWhlikl8+fPz/HHH58k2WabbXLTTTelXr161fav6vVaTbMqmD9/fv76178mSZo0aZKf/OQni/VRy5SK4cOHF4/POOOMKn833H777bP//vsnST777LP8+9//rtSunikF8+bNy+23354kad++ffF3wG8699xzs8EGGyRJLr744sXafbYBAMDSEJYqIQ888EDx+Oijj66yzxprrJGjjjoqyYJfbJ955pmVMTVYoQqFQh588MEkyWabbZbOnTtX2a9z587FL/UffPDBFAqFlTZHWGjh0txJ8s477yzWrp5Z1TRt2jRJMnPmzMXaFr4XWWuttar8QihJ2rVrl7322itJ8tRTT9kWmJXihBNOyFdffZU+ffpkt912W2J/r82sLtQypeaJJ57I22+/nST53e9+l7p16y7T+WqaVcVTTz2VDz/8MEly6KGHplGjRpXa1TKlZPbs2cXjjTbaqNp+G2+8cZXnqGdKxdtvv10MOXXv3r3ai8Lq1KmT7t27J0lGjRqV8ePHV2r32QYAAEtDWKqEPPfcc0mSxo0bZ/vtt6+236JfED3//PPf+bxgRRs/fnwmTpyYJEv8wnNh+4cffpj33nvvu54aLGbWrFnF46o+pFHPrErefPPNvPLKK0kWfAi+qNmzZ2fEiBFJki5dutS4cs/CWp41a1ZGjhz53UwW/p+77747Dz30UFq0aJH/+Z//WapzvDazulDLlJp77rknSVJRUVFcoSRJpk2blrfffjvTpk2r8Xw1zari1ltvLR4vvGhxUWqZUrIwwJQk7777brX9Fl4AVlFRke9///vF29UzpWLq1KnF4yXtaLBo+6IrZ/tsAwCApSUsVUIWLn+8ySab1Hh15qJfbn5zyWRYFfzrX/8qHn/zy/pvUu/UtqFDhxaPF26Duij1TKmbPn163n777Vx++eXZbbfdilsrnHLKKZX6vfXWW5k3b14StUzp+Oyzz3LyyScnqXr7hOp4baYU3XPPPdl8883TqFGjNG3aNN///vfTp0+fGlcLVsuUmhdffDFJ0qFDhzRt2jR33HFHttpqq7Rs2TIdO3ZMy5Yts+mmm+Z//ud/Kl10sJCaZlXw1VdfZfDgwUkWbAPVrVu3xfqoZUpJr169stZaayVZ8J554e91ixozZkwefvjhJEnv3r2L/RP1TOlo0qRJ8XjhClPVWbR90Rr22QYAAEtr2dZL5zszc+bMTJkyJcmCJWBrsvbaa6dx48b5+uuv85///GdlTA9WqAkTJhSPl1Tv66+/fvFYvbOyzZ8/PxdffHHxz4cffvhifdQzpWjgwIHVbumbJL///e/Tu3fvSrepZUrR6aefno8//ji77LJLjj322KU+Tz1Tihb9EidJxo0bl3HjxuXWW29Nz549M3DgwDRr1qxSH7VMKZk/f37eeOONJEmrVq1y8skn5+qrr16s31tvvZXf/va3GTx4cB5++OE0b9682KamWRXcd999+frrr5MkP/vZz1JRUbFYH7VMKWnVqlVuu+229OrVK88//3x22GGHnHLKKenYsWO++uqrPP/887nssssye/bsbLfddrnssssqna+eKRWbbLJJ6tWrlzlz5uQf//hHjX0Xbf/ggw+Kx+oZAIClZWWpErHontiLXkFRncaNGydZcLUbrGqWpd4X1nqi3ln5rrjiiuLS3T/5yU+q3CJVPbMq+eEPf5gRI0bkoosuWuxLH7VMqRk2bFhuvPHG1K1bN9dff32VX1RWRz1TSho1apQjjjgif/nLXzJs2LCMGTMmTzzxRM4666y0bNkySfLAAw/koIMOypw5cyqdq5YpJZ9//nnmz5+fJBk7dmyuvvrqtG3bNrfffnumTZuW6dOnZ+jQoencuXOSZPjw4TnmmGMqjaGmWRUsaQu+RC1Teg488MCMGjUq/fr1yyuvvJI+ffqkS5cu6d69ewYMGJBGjRrlyiuvzLBhwxbb3kw9UyoaN26cPfbYI0nyz3/+M3feeWeV/e68886MHTu2+OdFa1g9AwCwtKwsVSJmzpxZPK5pH+2FGjRokCSZMWPGdzYn+K4sS70vrPVEvbNyDR06NL///e+TJG3atMl1111XZT/1TCnq2bNnOnXqlGRBrb3zzju5++67M3jw4PTq1StXXnll9t9//0rnqGVKyezZs/PLX/4yhUIhv/71r7Plllsu0/nqmVLy4YcfVlpZZ6Hu3bvnxBNPzL777psxY8Zk6NChue6663LSSScV+6hlSsnClXaSBbXZqFGjPPPMM9l0002Lt//oRz/K008/nS5duuTVV1/N4MGD89JLL2WnnXYqnreQmqYUTZgwIc8++2ySpHPnzunYsWOV/dQypWb27Nm59dZb8+CDD6ZQKCzWPmnSpNx+++3ZcMMNc+CBB1ZqU8+UkgEDBuSpp57K3Llz06dPn7zzzjs56qij0rZt23z00Ue59dZbc/7556d+/fqZPXt2ksq1qJ4BAFhaVpYqEQ0bNiweL3yTX5NZs2YlSdZcc83vbE7wXVmWel9Y64l6Z+V5/fXXc/DBB2fu3Llp2LBh7rnnnrRp06bKvuqZUtS8efNsueWW2XLLLbPDDjvkiCOOyP33359bb7017777bg466KAMHDiw0jlqmVLyhz/8IW+88UY22GCD9O/ff5nPV8+UkqqCUguts846uffee1OvXr0kyTXXXFOpXS1TShatxyTp169fpaDUQmuuuWYuvPDC4p/vuuuuKsdQ05Si22+/vbiCWp8+fartp5YpJV9//XX22muvXHTRRZk2bVpOP/30/Pvf/86sWbPy+eef54knnkjXrl0zcuTI9OzZM5dffnml89UzpaRz58654YYbUrdu3cyZMyfnnHNO2rdvn/r166d9+/Y555xzUrdu3Up13LRp0+KxegYAYGkJS5WIRd/QL82Srwuv6FyaLfug1CxLvS969bJ6Z2UYP3589t5773z66aepU6dOBg0alB/96EfV9lfPrEp+/vOf57DDDsv8+fPzq1/9KtOmTSu2qWVKxRtvvJGLLrooyYLgyKJbIywt9cyqZKONNkr37t2TJOPGjcvEiROLbWqZUrJoPSbJ3nvvXW3fPffcM3XrLljM/OWXX65yDDVNKbrtttuSLFht5Kc//Wm1/dQypWTAgAEZNmxYkuSmm27KJZdcks022yz169fPWmutle7du+eZZ57J7rvvnkKhkN/+9rd59dVXi+erZ0rNMccck5deeikHH3xwpd8H69atmwMPPDCjR48urqadJGuvvXbxWD0DALC0bMNXIho2bJiWLVtm6tSpmTBhQo19P/300+Ib+fXXX39lTA9WqHbt2hWPl1Tv//nPf4rH6p3v2sSJE7PXXntl4sSJqaioyP/+7//moIMOqvEc9cyq5qCDDsrdd9+dr7/+Oo899lh69+6dRC1TOq644orMnj07G220UaZPn55BgwYt1ue1114rHj/99NP5+OOPkyQHHHBAGjdurJ5Z5Wy++eZ55JFHkizYtm+99dZL4rWZ0tKgQYO0bt06kydPTlJznTVs2DCtWrXKxx9/XOyfqGlK28iRI/Ovf/0rSbL//vtX+vL9m9QypaJQKOR///d/kyQdO3asdkW0unXr5oILLkjXrl0zf/78DBw4MFdccUUS9Uxp2m677XL//fdn7ty5+eijjzJ79ux873vfK64cdfvttxf7brHFFsVj9QwAwNISliohm2++eYYNG5Zx48Zl7ty5xaswv+mNN94oHv/gBz9YWdODFWbzzTcvHi9az1VR76wsU6ZMSffu3fPuu+8mWbCayVFHHbXE89Qzq5rWrVsXj99///3icceOHVOnTp3MmzdPLVOrFm6F8O6776ZXr15L7H/BBRcUj8ePH5/GjRt7bWaVU1FRUeXtaplSs8UWW+TZZ59NksybN6/GvgvbF/1sQ01Tym699dbicU1b8CVqmdIxadKk4orB2267bY19t99+++LxonWpnilldevWrTLINGrUqOLxjjvuWDz22QYAAEvLNnwlpGvXrkkWLP+66Jv9bxo6dGjxeJdddvnO5wUr2oYbbli8Wn7Req7KP/7xjyTJ9773vXTo0OG7nhpl6vPPP8+Pf/zj4lXEF198cU444YSlOlc9s6r58MMPi8eLLjNfv3794geML7zwQmbPnl3tGAtrvUGDBpWWvodS4bWZVc3C9yBJirWbqGVKz6LbUy+8yKAqX3zxRaZMmZJkQU0upKYpVXPmzCmuZtm6devsu+++NfZXy5SKRQOpc+fOrbHvnDlzqjxPPbOqmTdvXu6///4kC1aE2nnnnYttPtsAAGBpCUuVkJ49exaPb7755ir7zJ8/v3ilW/PmzbP77ruvjKnBClVRUVHc2uyNN97Iiy++WGW/F198sXiFz0EHHVTtFffwbUyfPj09evTI6NGjkyRnnXVWfve73y31+eqZVc0999xTPN5qq60qtS18L/LFF18UP3j8pgkTJmTIkCFJkj333DNNmzb9biZK2Ro4cGAKhUKNP/379y/2f+aZZ4q3L/zCxmszq5Lx48fnySefTJJsvPHGlYIlaplSc8ghhxSPBw8eXG2/wYMHp1AoJEl23XXX4u1qmlL16KOPFreM7N27d7WrvS+klikVLVq0yFprrZVkQTCkpsDUokGoDTfcsHisnlnV3HTTTfnggw+SJMcdd1zq1KlTqd1nGwAALJUCJWXXXXctJCnUrVu3MHz48MXaL7300kKSQpJC//79V/4EoQrjx48v1mWfPn2W6pw333yzUKdOnUKSQqdOnQrTp0+v1D59+vRCp06div8e3nrrre9g5pS7WbNmFfbee+9i/Z588snLNY56phTcfPPNhRkzZtTY5/LLLy/W+4YbbliYO3dupfapU6cWmjVrVkhSaN++fWHKlCmV2ufOnVs44IADimM888wzK/phwFLp37//EuvQazOl4G9/+1thzpw51bZ//PHHhW233bZYz5dddtlifdQypWbfffctJCmsscYahSFDhizW/tFHHxXatWtXSFKoX79+YcKECZXa1TSl6JBDDim+Fo8aNWqpzlHLlIpevXoV63fAgAFV9pk2bVph8803L/Z7/PHHK7WrZ0rJN987LOqpp54qrLnmmoUkhY4dO1b5OYjPNgAAWBoVhcL/u9SPkjBmzJjssssumTFjRpo0aZIzzzwzu+++e2bMmJFBgwblz3/+c5IFe2+PHDnSFQ/Uiueeey7jxo0r/nnKlCn57W9/m2TB1pD9+vWr1L9v375VjnPGGWfk4osvTpJsu+22+d3vfpeNN94477zzTi655JKMGTOm2O8Pf/jDd/BIKHeHHHJI8QqzPfbYI1deeWWNV0bWr18/HTt2rLJNPVPbOnTokC+//DKHHHJIunbtmo033jhNmjTJl19+mbFjx+avf/1rnn/++SQLavnhhx/OXnvttdg4N9xwQ/7rv/4ryYIVTs4666xstdVWmThxYq688so888wzSZJevXrljjvuWHkPEBYxYMCAnHfeeUkWrCzVrVu3Kvt5baa2dejQIXPmzMkhhxySLl26pEOHDllzzTUzZcqUPPvss7nhhhuKW5V17do1Q4YMSYMGDRYbRy1TSt56663stNNO+eyzz9KwYcOccsop2W+//bLmmmtmxIgRueiiizJhwoQkySWXXJLTTz99sTHUNKXk008/Tdu2bTNr1qxsueWWGTt27FKfq5YpBW+88Ua23377TJ8+PUlywAEHpE+fPtloo40yc+bMvPjii7nyyiuLK/HsueeexRV1FqWeKRVrr712dtttt/To0SNbbLFFGjRokA8++CCDBw/OX//618yfPz8tWrTI008/nW222abKMXy2AQDAEtV2WovF/e1vfyustdZaxSsbvvnTsWPHwttvv13b06SM9enTp9r6rOqnOvPmzSscc8wxNZ577LHHFubNm7cSHx3lZFnqOP/varTqqGdqW/v27Zeqjtu1a1d44oknahzr3HPPLVRUVFQ7xn777bfEVazgu7Q0K0sVCl6bqX1L+9p8yCGHFD799NNqx1HLlJphw4YV1llnnWrrsaKionD22WdXe76appRcd911xbq79NJLl+lctUypePLJJwutWrVa4nuOPfbYozBt2rQqx1DPlIrGjRvXWIdbbLFF4ZVXXlniOD7bAACgJlaWKlHvv/9+rrrqqjz88MOZMGFC6tevn0022SSHHXZYfvWrX6VRo0a1PUXKWN++fXPLLbcsdf8lvcw88sgj+fOf/5yXX345U6ZMSatWrbLDDjvkuOOOy7777vttpwvVqmkVqaq0b98+7733Xo191DO15c0338zDDz+c559/PuPGjcukSZMyderUrLnmmmnTpk1++MMfZv/998/hhx++VO8jhg8fnj/96U8ZNmxYJk2alObNm2ebbbbJ0UcfnV69eq2ERwTVW9qVpRby2kxtGTp0aIYOHZoXXngh7777bqZMmZIvvvgiTZo0yfrrr5+dd945ffr0SZcuXZZqPLVMKZk6dWquueaaPPDAAxk/fnxmz56dtm3bplu3bjnxxBOz7bbbLnEMNU0p2GWXXTJ8+PDUqVMnH3zwQdZbb71lHkMtUwqmTp2am266KY8++mhef/31fPbZZ6lbt27WXXfd7LDDDundu3cOPPDAJX4Wop6pbYMGDcoTTzyRESNG5KOPPspXX32V1q1bZ+utt85hhx2Wn/3sZ6lXr95SjeWzDQAAqiMsBQAAAAAAAAAAlIU1ansCAAAAAAAAAAAAK4OwFAAAAAAAAAAAUBaEpQAAAAAAAAAAgLIgLAUAAAAAAAAAAJQFYSkAAAAAAAAAAKAsCEsBAAAAAAAAAABlQVgKAAAAAAAAAAAoC8JSAAAAAAAAAABAWRCWAgAAAAAAAAAAyoKwFAAAAAAAAAAAUBaEpQAAAAAAAAAAgLIgLAUAAAAAAAAAAJQFYSkAAAAAAAAAAKAsCEsBAAAAAAAAAABlQVgKAAAAAAAAAAAoC8JSAAAAAAAAAABAWRCWAgAAoKhDhw6pqKhI3759a3sqK8TAgQNTUVGRioqKvPfee7U9nXTr1i0VFRXp1q1bbU8FAAAAAKAsCUsBAAAAAAAAAABlQVgKAAAAvoW+ffumoqIiHTp0qO2prDJKbcUvAAAAAKB81K3tCQAAAFA6BFe+W88++2xtTwEAAAAAoKxZWQoAAAAAAAAAACgLwlIAAAAAAAAAAEBZEJYCAABYDU2cODG///3vs91226VZs2apV69e1llnnWy11Vbp1atXBg4cmC+++GKx8zp06JCKior07dt3sbZnn302FRUVqaioyLPPPptCoZCbbropXbt2TcuWLbPWWmtlxx13zG233VbpvNmzZ+f6669P586d06JFizRt2jS77LJL7r777mrn/837qsnCfgMGDFiap6aS+fPn5+mnn85pp52WXXbZJa1atUq9evXSvHnz/PCHP8xpp52WDz74oMpzBwwYkIqKitxyyy1Jkvfff784l0V/FtWtW7dUVFSkW7dui4333nvvFc8ZOHBgkuTJJ5/MAQcckHXXXTcNGjTIhhtumOOPPz4TJkxY4mObOnVqTj/99Gy66aZZc801s84666R79+4ZPHhwkmTgwIHF+/s22y8OHjw4PXv2TLt27dKgQYM0bdo0G220UXbdddecc845GTFiRLHvwr/Xo48+unjbhhtuuNhzVt3f+QMPPJDDDjssG2ywQRo2bJjmzZunU6dOOe+88/Lpp59WO8e+ffumoqIiHTp0SJJ8+OGH+c1vfpOOHTumUaNGad26dXr06JHHHntsuZ8HAAAAAGDVULe2JwAAAMCKNWzYsOy///6LhaE++eSTfPLJJ3nttdcyaNCgtGrVKvvvv/9y3cecOXNy0EEH5e9//3ul219++eUcddRRGTlyZK666qp8+umn6dmzZ/7xj39U6jd8+PAMHz4848aNy5lnnrlcc1gRzj///Jx33nmL3f7555/n1Vdfzauvvprrrrsut99+ew4++OCVOrczzjgjF198caXb3nvvvVx//fW57777MnTo0PzgBz+o8tyxY8eme/fumTRpUvG2mTNnZsiQIRkyZEh++ctfpkuXLt9qfvPmzUuvXr1yzz33VLp99uzZ+eqrrzJ+/Pg899xzefTRRzNy5MhvdV+ffvppDj300Dz99NOVbp81a1ZGjRqVUaNG5dprr82DDz6Yzp071zjWyJEj06NHj3zyySfF22bMmJFHHnkkjzzySH7zm9/ksssu+1bzBQAAAABKl7AUAADAamTWrFk54ogj8sUXX6Rp06Y5/vjjs/vuu6dNmzaZPXt2xo8fn+HDhxdXF1pe55xzTl566aUceeSR6d27d9Zdd9289dZbGTBgQN58881cffXVOeCAA3LNNddk+PDhOf7443PwwQenZcuWeeWVV3LOOedk4sSJOffcc3PQQQdliy22WEHPwLKZO3du2rZtm4MPPjhdunTJRhttlIYNG+Y///lPhg8fnmuvvTZfffVVevfundGjR1cKJ/33f/93Dj300Jx99tl58MEHs9566+Xxxx9fIfP6y1/+kuHDh2e33XbLcccdl44dO+azzz7LrbfemltvvTWTJ0/OMccckxdeeGGxcz/77LPss88+xaDUz3/+8/Tu3TutW7fOuHHjctVVV+XPf/5zXn311W81x+uuu64YlOratWv69euXjTfeOI0bN87UqVPzz3/+M4899lg+//zz4jk77LBDxo4dmwcffDBnn312kuTxxx/PeuutV2nsDTfcsHg8a9as7LXXXhk9enTq1KmT3r17Z7/99suGG26YOXPm5B//+Ecuv/zyfPLJJ9lvv/0yZsyYtG/fvso5T58+PYcddlg+//zz/P73v89+++2XBg0a5KWXXspFF12Ujz76KJdffnk22GCDnHzyyd/q+QEAAAAASpOwFAAAwGrk+eefz8SJE5Mkd9xxx2IrR3Xu3Dm9evXKFVdckenTpy/3/bz00ku58sorKwVKtttuu3Tr1i0dO3bMl19+md69e2fKlCm5//7707Nnz0r9OnXqlG233Tbz5s3Ln//851x11VXLPZdvo1+/funfv3/q1atX6fbtttsuBx10UE488cR07tw5H374Yf7whz9U2mKwTZs2adOmTZo3b54kqVevXrbccssVMq/hw4fnF7/4RW644YZKW/ntueeeqV+/fm688ca8+OKLGTNmTLbddttK55533nnFGvjm39H222+fQw89NIccckgefPDBbzXHhdso7rTTTnnmmWdSt27ljxj22muv/OY3v8m0adOKtzVu3DhbbrllpZWmOnbsWNweryrnn39+Ro8enebNm2fIkCHZfvvtK7V37do1Rx55ZLp06ZKPPvooZ555Zv76179WOdbkyZPz2WefZciQIfnRj35UvH3HHXfMIYcckp122ikTJkzIWWedVQyYAQAAAACrlzVqewIAAACsOB9//HHxeNEwyDfVrVs3a6211nLfz0477VTlyjvrrrtucbu6yZMn5/DDD68UlFpo6623TteuXZMs2DawtnTo0GGxoNSi2rVrl9/+9rdJkr/97W8pFAorZV5t27bNNddcUykotdBpp51WPP7mczdr1qwMHDgwyYJVnKr6O6pTp05uuOGGNGzY8FvNcWGt7bzzzosFpRbVokWL5b6Pr776Kn/605+SJBdccMFiQamF2rdvn3POOSdJcs899+Trr7+udszjjjuuyn8b6623XnH7va+//jq33HLLcs8bAAAAAChdwlIAAACrkbZt2xaPb7755u/sfo444ohq27bZZptl6vfuu++uuIl9S1988UXGjx+f119/Pa+99lpee+21NGrUqFLbynDooYemQYMGVbZtuummadKkSZLFn7uRI0fms88+S5L87Gc/q3b8ddZZJz/+8Y+/1RwX1trf//73TJky5VuNVZ2hQ4cWt/E79NBDa+y7MAA1Z86cjBo1qtp+Rx99dLVtBx98cHGlsCFDhizjbAEAAACAVYGwFAAAwGqka9eu2WijjZIkp5xySnbcccdcdNFFef755zN79uwVdj8dO3astm1h2GRp+3355ZcralrL5f3338+JJ56YDh06pFmzZtloo42y5ZZbZquttspWW22VX/7yl8W+31Uo6Js222yzGtvXXnvtJIs/d6+99lrxuLpVmBbq1KnTcs5ugT59+iRJxo0bl0022STHHHNM7rzzzkyYMOFbjbuoRbfra9u2bSoqKqr9WXQLxEVXWFtU/fr1K4X5vqlevXrFbQ3Hjh27gh4FAAAAAFBKhKUAAABWI/Xq1cvf//73/OAHP0iSvPzyyznzzDPTtWvXNG/ePPvss0/uuOOOzJs371vdz8LVlqqyxhprLFO/+fPnf6u5fBuPPvpoNt988/zf//t/8/777y+x/4wZM1bCrGp+3pL//7n75t/jp59+Wjxu3bp1jWMsqX1JjjnmmJx55pmpW7duPv/889x8883p3bt31l9//WyyySY59dRTv/WqYZ988slynTd9+vQqb2/RokXq1KlT47nrrLNOkmTatGnLdd8AAAAAQGkTlgIAAFjNbL755hk7dmwGDx6cY445JptsskmSBUGfxx9/PEceeWR22mmn5Q6irC6mTJmS3r17Z/r06WnSpEkGDBiQF154IZ988klmzZqVQqGQQqGQp556qnhOoVCoxRmXngsvvDDjxo3LhRdemD322KMY8nrnnXdy+eWXZ7PNNsv111+/3OMvGgYbPXp0xo4du1Q/PXv2rHK8ioqK5Z4LAAAAALB6qFvbEwAAAGDFq1OnTnr27FkMjXz00Ud57LHH8qc//SmjRo3KqFGjctxxx2Xw4MG1O9FqLLo6VU0rT3399dfLfR/33ntvPvvssyTJ4MGDs9dee1XZb1VaYWjh9nxJMnny5Bq3QZw8efIKuc/27dvnzDPPzJlnnpk5c+bk5Zdfzt13350bbrghM2fOzH//939np512Km5vtyxatmxZPG7dunXatWv3reY6derUzJs3r8bVpSZNmpRkwSpUAAAAAMDqx8pSAAAAZaBt27Y5+uij88ILL2S77bZLkjz00EMrbVu5ZdW0adPi8aJby33TW2+9tdz38frrrydZEIqpLiiVJCNHjqxxnFJarWiLLbYoHo8aNarGvkt6XMujXr162XnnnXPllVfmjjvuSLJgNa577723Ur+lfc4WDVg9//zz33p+s2fPzquvvlpt+9y5c/PKK68kSbbccstvfX8AAAAAQOkRlgIAACgj9erVy2677ZZkQTBk4cpKpaZDhw7F45pCPXfeeedy38fcuXOTJDNnzqx29arp06fntttuq3Gchg0bJklmzZq13HNZUTp16pRmzZolSW6//fZq+02aNCmPP/74dzqXPffcs3g8ZcqUSm0Ln7Ok5udtr732Km7td/XVV6+QbRBvueWWatsGDx5cDOfVFKADAAAAAFZdwlIAAACrkWHDhmXcuHHVts+ePTtDhw5NkjRp0iStW7deWVNbJmuvvXa23nrrJMnNN99c5VZ4zz33XK666qrlvo/vf//7SRYEou6+++7F2ufNm5d+/fpl4sSJNY7Ttm3bJMknn3ySL7/8crnnsyI0bNgwRx11VJLk5ZdfrvL5mT9/fo477rjMnDnzW93X7bffXgycVeWJJ54oHm+44YaV2hY+Z0nyzjvvVDtG8+bN86tf/SpJMnz48Pz617+ucVvGSZMm5cYbb6xx3tddd12ee+65xW7/+OOPc9pppyVJGjVqlD59+tQ4DgAAAACwaqpb2xMAAABgxXnqqadywQUXZNddd02PHj2y9dZbp3Xr1pkxY0beeuutXH/99Rk9enSS5Nhjj03duqX7a+EJJ5yQ4447LpMmTcquu+6ac845J5tuummmTZuWhx9+ONdee206deqU4cOHL9f4hx9+eM4888zMmjUrRx99dF555ZV07949zZo1y+uvv55rrrkmo0aNyi677FLjFnA777xzkgUhpP/6r//KiSeemFatWhXbN9lkk+Wa3/IaMGBA7rnnnnz88cc55ZRTMmrUqBx55JFp3bp1xo0bl6uuuirDhw/PjjvumBEjRiRZvq0Ef/7zn+e0007LT37yk+y8887ZeOON07Bhw0yaNClPPvlkrrvuuiQLQnlHHnlkpXO33XbbNGzYMDNnzsw555yTevXqpX379lljjQXXdH3ve9/LmmuumSQ5//zzM3To0Lz00ku56qqr8uyzz+YXv/hFfvjDH6Zx48b59NNP8/rrr2fIkCF59NFHs9VWW6Vfv35Vzrl169Zp1KhRunfvnl//+tfZb7/90qBBg4wYMSJ/+MMfisG4Cy64IG3atFnm5wQAAAAAKH2l+6k4AAAAy2X+/PkZOnRocQWpqhx00EG56KKLVuKsll2/fv3y6KOP5oEHHsi//vWv9OrVq1L7Vlttlfvuu6/SKkXLol27drnuuuvSr1+/zJw5M5dcckkuueSSSn1++tOf5he/+EWNW7Ltscce6dy5c1588cXccccdueOOOyq1r4it45ZFixYt8thjj6V79+6ZPHlybrvttsW2Euzbt2923XXXYlhq0W3xlsWkSZNy3XXXFYNR39SsWbMMGjQo66+/fqXbmzZtmpNOOimXXnppRo8enb333rtS+zPPPJNu3bolSRo0aJAnn3wyffv2zf33359XX321uNpUVdZaa61q2xo1apR77703++67by666KIq/w2cdNJJ+c1vflPtGAAAAADAqs02fAAAAKuR0047Lffdd1+OP/74dO7cORtssEEaNmyYhg0bpkOHDjn88MPz0EMP5YEHHiiu3FOq1lhjjdx7773505/+lB122CGNGzdO48aNs/XWW+fCCy/MSy+9lHXXXfdb3cfRRx+dYcOGpWfPnmndunXq1auXtm3bZp999sldd92VQYMGpU6dOkuc5xNPPJGzzz4722yzTZo0abJcKzWtSNtss03+9a9/5dRTT833v//9NGjQIK1atcruu++eO+64IzfffHO++OKLYv9mzZot83289tprueSSS3LAAQdk8803T8uWLVOnTp00b948nTt3Tv/+/fPmm29mn332qfL8iy++OH/5y1+y6667pkWLFjU+z02bNs19992XYcOGpV+/ftl0003TtGnT1K1bNy1atMgOO+yQE044IY888kiefPLJGufdqVOnjB49OieddFJxNayWLVtmn332ySOPPPKttnYEAAAAAEpfRWFlX+IKAAAA1Lp+/frlpptuSrt27fKf//yntqfznerbt29uueWWtG/fPu+9915tTwcAAAAAqEVWlgIAAIAyM2PGjDz44INJks6dO9fybAAAAAAAVh5hKQAAAFjNvPPOO6luIel58+bl+OOPz5QpU5Ikffr0WZlTAwAAAACoVXVrewIAAADAinXBBRdkxIgROeKII7LTTjulTZs2mTFjRv75z3/mL3/5S0aPHp0k2WuvvdKjR49ani0AAAAAwMojLAUAAACroX//+9/p379/te277LJLBg0alIqKipU4KwAAAACA2iUsBQAAAKuZM844Ix07dsyQIUPy3nvvZfLkyZkzZ05atmyZTp065ac//WmOOOKIrLHGGrU9VQAAAACAlaqiUCgUansSAAAAAAAAAAAA3zWXkAIAAAAAAAAAAGVBWAoAAAAAAAAAACgLwlIAAAAAAAAAAEBZEJYCAAAAAAAAAADKgrAUAAAAAAAAAABQFoSlAAAAAAAAAACAsiAsBQAAAAAAAAAAlAVhKQAAAAAAAAAAoCwISwEAAAAAAAAAAGVBWAoAAAAAAAAAACgLwlIAAAAAAAAAAEBZEJYCAAAAAAAAAADKgrAUAAAAAAAAAABQFoSlAAAAAAAAAACAsiAsBQAAAAAAAAAAlAVhKQAAAAAAAAAAoCwISwEAAAAAAAAAAGVBWAoAAAAAAAAAACgLwlIAAAAAAAAAAEBZ+P8ABqeYnmWhYsEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 2400x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spikingjelly import visualizing\n",
    "\n",
    "s_list = torch.cat((record[0 , :].reshape(-1,1) , record[1 , :].reshape(-1 , 1)) , dim = 1)\n",
    "s_list.shape\n",
    "\n",
    "figsize = (12, 8)\n",
    "dpi = 200\n",
    "\n",
    "visualizing.plot_1d_spikes(spikes=s_list.detach().numpy(), title='membrane sotentials', xlabel='simulating step',\n",
    "                        ylabel='neuron index', figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the values\n",
    "\n",
    "plt.plot(error , label='error')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Plot of Tensor Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "spikes = torch.tensor(PoissonSpike(np.random.random(100),time=T,dt=1 , max_freq = 2500 , min_freq = 1800).spikes).float()\n",
    "network = test(layer1_number = 100)\n",
    "\n",
    "epoch_record = []\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = 1e-3 )\n",
    "losses = []\n",
    "N = []\n",
    "error = []\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    network.train()\n",
    "    loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for t in range (T):\n",
    "\n",
    "        out_fr , layers_spikes , w = network(spikes[: , t])\n",
    "        \n",
    "        if t == 0 :\n",
    "\n",
    "            record = layers_spikes[0].reshape(-1 , 1)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            record = torch.cat((record , layers_spikes[0].reshape(-1 , 1)) , dim = 1)\n",
    "\n",
    "\n",
    "\n",
    "    loss1 , N1 = coincidence_single_profile_cython(record[0 , :], record[1 , :], 0 , 99 , max_tau = 2)\n",
    "    loss2 , N2 = coincidence_single_profile_cython(record[1 , :], record[0 , :], 0 , 99 , max_tau = 2)\n",
    "    loss = loss1 + loss2\n",
    "\n",
    "\n",
    "    # Define the Gaussian kernel\n",
    "    kernel_size = 5\n",
    "    sigma = 1.0\n",
    "    kernel = torch.exp(-(torch.arange(kernel_size) - kernel_size // 2) ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    # Normalize the kernel\n",
    "    kernel = kernel / kernel.sum()\n",
    "    \n",
    "    \n",
    "    # Add a dimension to match the tensor shape\n",
    "    kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply the convolution\n",
    "    vector1 = (F.conv1d(record[0 , :].unsqueeze(0).unsqueeze(0), kernel)).squeeze()\n",
    "    vector2 = (F.conv1d(record[1 , :].unsqueeze(0).unsqueeze(0), kernel)).squeeze()\n",
    "    \n",
    "    squared_diffs = (vector1 - vector2) ** 2\n",
    "\n",
    "    \n",
    "    # Calculate the MSE\n",
    "    mse = squared_diffs.mean()\n",
    "\n",
    "    loss = mse + \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"MSE:\", mse) \n",
    "    error.append(mse.item())\n",
    "    print(\"total number of spikes\" , N1+N2)\n",
    "    N.append(N1+N2)\n",
    "    losses.append(loss1+loss2)\n",
    "    mse.backward(retain_graph = True)\n",
    "    print(\"synchrony loss\" ,  loss1 + loss2)\n",
    "    print (record)\n",
    "    optimizer.step()\n",
    "    functional.reset_net(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(batch_activation , max_tau):\n",
    "\n",
    "\n",
    "        s = []\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(len(batch_activation)): # which Layer\n",
    "            for j in range(0 , len(batch_activation[i]) - 1): # first Neuron\n",
    "                for k in range( j+1 , len(batch_activation[i])): # second Neuron\n",
    "\n",
    "                    neuron1 = batch_activation[i][j]\n",
    "                    neuron2 = batch_activation[i][k]\n",
    "\n",
    "                    for l in range((batch_activation[i][k]).size(0)):\n",
    "\n",
    "                        spiketrain1 = batch_activation[i][j][l , :]\n",
    "                        spiketrain2 = batch_activation[i][k][l , :]\n",
    "\n",
    "                        s.append(calc(spiketrain1 , spiketrain2 ، j , k , max_tau))\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                        \n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
